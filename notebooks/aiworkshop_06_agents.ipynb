{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.6: Agents, Function Calling, Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mermaid Live Editor: https://mermaid.live/edit\n",
    "- Emerging Patterns in Building AI Agents: https://martinfowler.com/articles/gen-ai-patterns/\n",
    "    - Hier von AI-Anwendung Code erstellen lassen als Beispiele!\n",
    "\n",
    "- Bildquelle1 multiagentsystems profit\n",
    "    - https://x.com/tom_doerr/status/1906206992424693884/photo/1\n",
    "\n",
    "- OpenAI Agents: https://github.com/openai/openai-agents-python?tab=readme-ov-file / https://openai.github.io/openai-agents-python/\n",
    "- Agent Patterns by Openai: https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Multiagentsystems profit](assets/image04_multiagentsystemsprofit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from aiworkshop_utils.standardlib_imports import os, json, base64, logging, Optional, List, Literal, pprint, glob, asyncio, datetime, date, time, timezone, ZoneInfo, uuid, dataclass\n",
    "from aiworkshop_utils.thirdparty_imports import AutoTokenizer, load_dotenv, requests, BaseModel, Field, pd, cosine_similarity, plt, np, DataType, MilvusClient, DDGS, rprint\n",
    "from aiworkshop_utils.custom_utils import show_pretty_json, encode_image\n",
    "from aiworkshop_utils.jupyter_imports import Markdown, HTML, JSON, display, widgets\n",
    "from aiworkshop_utils.openai_imports import OpenAI, Agent, Runner, InputGuardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered, OpenAIChatCompletionsModel, AsyncOpenAI, set_tracing_disabled, ModelSettings, function_tool, trace, ResponseContentPartDoneEvent, ResponseTextDeltaEvent, RawResponsesStreamEvent, TResponseInputItem, ItemHelpers, MessageOutputItem, RunContextWrapper, input_guardrail, output_guardrail\n",
    "from aiworkshop_utils import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ Erster Agent mit Agents-SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was ist ein Agent?\n",
    "    - Ein Agent ist ein LLM, das mit Instruktionen und Tools konfiguriert ist.\n",
    "    - \"name\" -> Name des Agenten\n",
    "    - \"instructions\" -> Entwickler Message bzw. Systemprompt\n",
    "    - \"model\" -> welches LLM wird benutzt (in \"model_settings\" kann man z.B. Temperatur ändern)\n",
    "    - \"tools\" -> Liste [] an Tools, die das LLM benutzen kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_base = config.OLLAPI_ENDPOINT_BASE\n",
    "\n",
    "model = config.OMODEL_LLAMA3D2 #model = config.OMODEL_DEEPSEEK Deepseek does not support tools! GEMMA does not support tools!\n",
    "set_tracing_disabled(True) # um Message \"OPENAI_API_KEY is not set, skipping trace export\" zu vermeiden\n",
    "\n",
    "model = OpenAIChatCompletionsModel( \n",
    "    model=model,\n",
    "    openai_client=AsyncOpenAI(base_url=endpoint_base, api_key=\"fake-key\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_agent = Agent(\n",
    "    name=\"Assistant\", \n",
    "    instructions=\"You are a helpful assistant\", \n",
    "    model=model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(first_agent, \"Write a haiku for Eisenstadt, Burgenland, Austria.\") # await, weil es in Jupyter einen existing Event Loop gibt\n",
    "print(\"-----result\")\n",
    "print(result)\n",
    "print()\n",
    "print(\"-----result.input\")\n",
    "print(result.input)\n",
    "print()\n",
    "print(\"-----result-final_output\")\n",
    "print(result.final_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ Ein Tool erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was ist Function Calling?\n",
    "    - Function Calling bedeutet, dass LLMs nicht nur Text generieren, sondern gezielt externe Funktionen aufrufen, um an strukturierte Daten zu gelangen oder Aktionen auszuführen (Daten aus DB abfragen, Bestellung aufgeben, ...)\n",
    "    - Externe Funktionen werden definiert\n",
    "    - Dem Model wird beschrieben, was die Funktion tut un welche Parameter benötigt werden\n",
    "    - Wenn eine Nutzeranfrage so eine Aktion erfordert, gibt das LLM nicht die Antwort direkt zurück, sondern ruft die passende Funktion inklusive notwendige Parameter auf\n",
    "    - Externe Funktion liefert Ergebnis und as Modell gibt die passende Antwort an die/den User:in weiter\n",
    "    - Z.B. Anfrage „Wie wird das Wetter morgen in Wien?“\n",
    "    - Modell versteht, es muss Funktion aufrufen:\n",
    "```\n",
    "{\n",
    "  \"function\": \"get_weather_forecast\",\n",
    "  \"parameters\": {\n",
    "    \"location\": \"Wien\",\n",
    "    \"date\": \"2025-04-05\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "- Was ist ein Tool?\n",
    "    - Function Calling ist die technische Fähigkeit eines LLM, eine externe Funktion aktiv aufrufen zu können\n",
    "    - Ein Tool ist solche externe Funktion oder eine Ressource\n",
    "    - Ein \"Agent\" wird instruiert, wann und wie Tools verwendet werden sollen\n",
    "    - Das Modell entscheidet autonom, welches Tool es verwendet\n",
    "    - Das Tool liefert eine Antwort in strukturierter Form zurück"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherData(BaseModel):\n",
    "    place: str\n",
    "    # time: datetime\n",
    "    interval: int\n",
    "    temperature_2m: float\n",
    "    wind_speed_10m: float\n",
    "\n",
    "@function_tool\n",
    "def get_weather(latitude, longitude) -> WeatherData:\n",
    "    \"\"\"\n",
    "    Fetches the weather for a given location using the Open-Meteo API.\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        f\"https://api.open-meteo.com/v1/forecast?\"\n",
    "        f\"latitude={latitude}&longitude={longitude}\"\n",
    "        f\"&current=temperature_2m,wind_speed_10m\"\n",
    "        f\"&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Ensure we catch any HTTP errors\n",
    "    data = response.json()\n",
    "    return data[\"current\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_agent = Agent(\n",
    "    name=\"Weather Agent\", \n",
    "    instructions=\"You are a helpful weather assisting agent\", \n",
    "    tools=[get_weather],  # Register the tool with the agent\n",
    "    model=model\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(weather_agent, \"What is the weather in Eisenstadt, Burgenland, Austria?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🏋️ **[ÜBUNG_2.6.01]** Multi-Tool-Aufruf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Erstelle ein oder mehrere weitere Tools neben dem Wetter-API-Tool\n",
    "- Erstelle einen Agenten, der zwei oder mehr dieser Tools bei seiner Instanzierung als Liste mitnimmt\n",
    "- Versuche, diesen Agenten mit nur einer User-Message zum Aufrufen mehrerer Tools zu bringen\n",
    "    - Du kannst tool_choice als Paramter bei der Agentenerstellung auf required setzen mit dem ModelSettings-Objekt\n",
    "    - um unendliche Loops zu vermeiden, wird nach einem Tool-Aufruf dieser Parameter wieder auf auto gesetzt\n",
    "```\n",
    "# Set model settings with tool_choice\n",
    "model_settings = ModelSettings(\n",
    "    tool_choice=\"required\",  # or \"auto\", \"none\", or a specific tool name\n",
    ")\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You are a helpful weather assisting agent.\",\n",
    "    model=model\n",
    "    model_settings=model_settings,\n",
    "    tools=[weather_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es gibt bei Agent-Erstellung auch den Parameter tool_use_behavior= default -> \"run_llm_again\", other choice -> \"stop_on_first_tool\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔓 **[LÖSUNG_2.7.01]** Multi-Tool Aufruf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimale Tools und Looping Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckResult(BaseModel):\n",
    "    is_good: bool\n",
    "\n",
    "# Agent 1: Text schreiben\n",
    "writer = Agent(\n",
    "    name=\"writer\",\n",
    "    instructions=\"Schreibe einen sehr kurzen Satz über Technologie.\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Agent 2: Qualität prüfen\n",
    "checker = Agent(\n",
    "    name=\"checker\",\n",
    "    instructions=\"Bewerte, ob der Text klar und verständlich ist. Wenn nicht, sag is_good = False.\",\n",
    "    output_type=CheckResult,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Agent 3: Orchestrator mit Schleife\n",
    "looping_agent = Agent(\n",
    "    name=\"looping_agent\",\n",
    "    instructions=(\n",
    "        \"Erzeuge einen Satz über Technologie mit dem 'writer'-Tool.\\n\"\n",
    "        \"Lass ihn dann vom 'checker'-Tool bewerten.\\n\"\n",
    "        \"Wenn das Ergebnis nicht gut ist, rufe den 'writer' erneut auf, bis das Ergebnis gut ist.\"\n",
    "    ),\n",
    "    tools=[\n",
    "        writer.as_tool(\"generate_text\", tool_description=\"Erzeuge einen Satz.\"),\n",
    "        checker.as_tool(\"check_text\", tool_description=\"Prüfe den Satz.\"),\n",
    "    ],\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await Runner.run(looping_agent, \"Bitte schreibe einen guten Satz über Technologie.\")\n",
    "print(\"✅ Final result:\\n\", result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeworkOutput(BaseModel):\n",
    "    is_homework: bool\n",
    "    reasoning: str\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"Check if the user is asking about homework.\",\n",
    "    model=model,\n",
    "    output_type=HomeworkOutput,\n",
    ")\n",
    "\n",
    "math_tutor_agent = Agent(\n",
    "    name=\"Math Tutor\",\n",
    "    handoff_description=\"Specialist agent for math questions\",\n",
    "    instructions=\"You provide help with math problems. Explain your reasoning at each step and include examples\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "history_tutor_agent = Agent(\n",
    "    name=\"History Tutor\",\n",
    "    handoff_description=\"Specialist agent for historical questions\",\n",
    "    instructions=\"You provide assistance with historical queries. Explain important events, people, and context clearly.\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "\n",
    "async def homework_guardrail(ctx, agent, input_data):\n",
    "    result = await Runner.run(guardrail_agent, input_data, context=ctx.context)\n",
    "    final_output = result.final_output_as(HomeworkOutput)\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output,\n",
    "        tripwire_triggered=not final_output.is_homework,\n",
    "    )\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"Triage Agent\",\n",
    "    instructions=\"You determine which agent to use based on the user's homework question\",\n",
    "    model=model,\n",
    "    handoffs=[history_tutor_agent, math_tutor_agent],\n",
    "    input_guardrails=[\n",
    "        InputGuardrail(guardrail_function=homework_guardrail),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = await Runner.run(triage_agent, \"I need help for my homework: Who was again the first president of Columbia?\")\n",
    "    print(result.final_output)\n",
    "except InputGuardrailTripwireTriggered:\n",
    "    print(\"It seems your question doesn't relate to homework. Please ask a homework-related question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = await Runner.run(triage_agent, \"What is life?\")\n",
    "    print(result.final_output)\n",
    "except InputGuardrailTripwireTriggered:\n",
    "    print(\"It seems your question doesn't relate to homework. Please ask a homework-related question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m\")\n",
    "\n",
    "# 1. Create Internet Search Tool\n",
    "\n",
    "@function_tool\n",
    "def get_news_articles(topic):\n",
    "    print(f\"Running DuckDuckGo news search for {topic}...\")\n",
    "    \n",
    "    # DuckDuckGo search\n",
    "    ddg_api = DDGS()\n",
    "    results = ddg_api.text(f\"{topic} {current_date}\", max_results=5)\n",
    "    if results:\n",
    "        news_results = \"\\n\\n\".join([f\"Title: {result['title']}\\nURL: {result['href']}\\nDescription: {result['body']}\" for result in results])\n",
    "        print(news_results)\n",
    "        return news_results\n",
    "    else:\n",
    "        return f\"Could not find news results for {topic}.\"\n",
    "    \n",
    "# 2. Create AI Agents\n",
    "\n",
    "# News Agent to fetch news\n",
    "news_agent = Agent(\n",
    "    name=\"News Assistant\",\n",
    "    instructions=\"You provide the latest news articles for a given topic using DuckDuckGo search.\",\n",
    "    tools=[get_news_articles],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Editor Agent to edit news\n",
    "editor_agent = Agent(\n",
    "    name=\"Editor Assistant\",\n",
    "    instructions=\"Rewrite and give me as news article ready for publishing.\" \n",
    "    \"Each News story in separate section.\"\n",
    "    \"**Do not repeat or duplicate the articles.**\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# 3. Create workflow\n",
    "\n",
    "async def run_news_workflow(topic):\n",
    "    print(\"Running news Agent workflow...\")\n",
    "    \n",
    "    # Step 1: Fetch news\n",
    "    news_response = await Runner.run(\n",
    "        news_agent,\n",
    "        f\"Get me the news about {topic} on {current_date}\"\n",
    "    )\n",
    "    \n",
    "    # Access the content from RunResult object\n",
    "    raw_news = news_response.final_output\n",
    "    \n",
    "    # Step 2: Pass news to editor for final review\n",
    "    edited_news_response = await Runner.run(\n",
    "        editor_agent,\n",
    "        raw_news\n",
    "    )\n",
    "    \n",
    "    # Access the content from RunResult object\n",
    "    edited_news = edited_news_response.final_output\n",
    "    \n",
    "    print(\"Final news article:\")\n",
    "    print(edited_news)\n",
    "    \n",
    "    return \"success\"\n",
    "\n",
    "# Example of running the news workflow for a given topic\n",
    "await run_news_workflow(\"AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ Agent Patterns Beispiele von (https://github.com/openai/openai-agents-python/tree/main/examples/agent_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simples Logging Beispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create logs directory if it doesn't exist\n",
    "log_dir = Path(\"logs\")\n",
    "log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 2. Create a unique log file name with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = log_dir / f\"story_flow_{timestamp}.log\"\n",
    "\n",
    "# 3. Configure logging to write to this file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()  # Also print to Jupyter output cell\n",
    "    ],\n",
    "    force=True  # Needed in Jupyter to reset any previous config\n",
    ")\n",
    "\n",
    "logging.info(f\"Logging to file: {log_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example demonstrates a deterministic flow, where each step is performed by an agent.\n",
    "1. The first agent generates a story outline\n",
    "2. We feed the outline into the second agent\n",
    "3. The second agent checks if the outline is good quality and if it is a scifi story\n",
    "4. If the outline is not good quality or not a scifi story, we stop here\n",
    "5. If the outline is good quality and a scifi story, we feed the outline into the third agent\n",
    "6. The third agent writes the story\n",
    "\"\"\"\n",
    "\n",
    "# Define agents\n",
    "story_outline_agent = Agent(\n",
    "    name=\"story_outline_agent\",\n",
    "    instructions=\"Generate a very short story outline based on the user's input.\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "class OutlineCheckerOutput(BaseModel):\n",
    "    good_quality: bool\n",
    "    is_scifi: bool\n",
    "\n",
    "outline_checker_agent = Agent(\n",
    "    name=\"outline_checker_agent\",\n",
    "    instructions=\"Read the given story outline, and judge the quality. Also, determine if it is a scifi story.\",\n",
    "    output_type=OutlineCheckerOutput,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "story_agent = Agent(\n",
    "    name=\"story_agent\",\n",
    "    instructions=\"Write a short story based on the given outline.\",\n",
    "    output_type=str,\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Async function to run the workflow\n",
    "async def run_story_flow():\n",
    "    input_prompt = input(\"What kind of story do you want? \")\n",
    "    logging.info(\"Received user input for story type.\")\n",
    "\n",
    "    with trace(\"Deterministic story flow\"):\n",
    "        # 1. Generate an outline\n",
    "        logging.info(\"Invoking story_outline_agent.\")\n",
    "        outline_result = await Runner.run(\n",
    "            story_outline_agent,\n",
    "            input_prompt,\n",
    "        )\n",
    "        logging.info(f\"Outline generated: {outline_result.final_output}\")\n",
    "\n",
    "        # 2. Check the outline\n",
    "        logging.info(\"Invoking outline_checker_agent.\")\n",
    "        outline_checker_result = await Runner.run(\n",
    "            outline_checker_agent,\n",
    "            outline_result.final_output,\n",
    "        )\n",
    "\n",
    "        # 3. Gate logic\n",
    "        checker_output = outline_checker_result.final_output\n",
    "        if not checker_output.good_quality:\n",
    "            logging.warning(\"Outline is not good quality. Stopping workflow.\")\n",
    "            return\n",
    "        if not checker_output.is_scifi:\n",
    "            logging.warning(\"Outline is not a scifi story. Stopping workflow.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"Outline is good quality and a scifi story. Proceeding to story generation.\")\n",
    "\n",
    "        # 4. Write the story\n",
    "        logging.info(\"Invoking story_agent.\")\n",
    "        story_result = await Runner.run(\n",
    "            story_agent,\n",
    "            outline_result.final_output,\n",
    "        )\n",
    "        logging.info(f\"Story generated: {story_result.final_output}\")\n",
    "\n",
    "# Run the async function in Jupyter\n",
    "await run_story_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handoffs and Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define agents\n",
    "french_agent = Agent(\n",
    "    name=\"french_agent\",\n",
    "    instructions=\"You only speak French\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You only speak Spanish\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "english_agent = Agent(\n",
    "    name=\"english_agent\",\n",
    "    instructions=\"You only speak English\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"triage_agent\",\n",
    "    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n",
    "    handoffs=[french_agent, spanish_agent, english_agent],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Conversation ID for tracing\n",
    "conversation_id = str(uuid.uuid4().hex[:16])\n",
    "\n",
    "# Async function\n",
    "async def run_routing_flow():\n",
    "    msg = input(\"Hi! We speak French, Spanish and English. How can I help? \")\n",
    "    agent = triage_agent\n",
    "    inputs: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "    while True:\n",
    "        with trace(\"Routing example\", group_id=conversation_id):\n",
    "            print(f\"\\n🧠 Current agent: {agent.name}\")\n",
    "            result = Runner.run_streamed(\n",
    "                agent,\n",
    "                input=inputs,\n",
    "            )\n",
    "            response = \"\"\n",
    "\n",
    "            async for event in result.stream_events():\n",
    "                if not isinstance(event, RawResponsesStreamEvent):\n",
    "                    continue\n",
    "                data = event.data\n",
    "                if isinstance(data, ResponseTextDeltaEvent):\n",
    "                    print(data.delta, end=\"\", flush=True)\n",
    "                    response += data.delta\n",
    "                elif isinstance(data, ResponseContentPartDoneEvent):\n",
    "                    print(\"\\n✅ Done\\n\")\n",
    "\n",
    "        inputs = result.to_input_list()\n",
    "        user_msg = input(\"Enter a message (or type 'exit' to quit): \")\n",
    "        if user_msg.lower() == \"exit\":\n",
    "            print(\"👋 Conversation ended.\")\n",
    "            break\n",
    "        inputs.append({\"content\": user_msg, \"role\": \"user\"})\n",
    "        agent = result.current_agent  # update agent if handed off\n",
    "\n",
    "# Run in Jupyter\n",
    "await run_routing_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agenten als Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define translation agents\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You translate the user's message to Spanish\",\n",
    "    handoff_description=\"An english to spanish translator\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "french_agent = Agent(\n",
    "    name=\"french_agent\",\n",
    "    instructions=\"You translate the user's message to French\",\n",
    "    handoff_description=\"An english to french translator\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "italian_agent = Agent(\n",
    "    name=\"italian_agent\",\n",
    "    instructions=\"You translate the user's message to Italian\",\n",
    "    handoff_description=\"An english to italian translator\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Orchestrator agent that uses the others as tools\n",
    "orchestrator_agent = Agent(\n",
    "    name=\"orchestrator_agent\",\n",
    "    instructions=(\n",
    "        \"You are a translation agent. You use the tools given to you to translate. \"\n",
    "        \"If asked for multiple translations, you call the relevant tools in order. \"\n",
    "        \"You never translate on your own, you always use the provided tools.\"\n",
    "    ),\n",
    "    tools=[\n",
    "        spanish_agent.as_tool(\n",
    "            tool_name=\"translate_to_spanish\",\n",
    "            tool_description=\"Translate the user's message to Spanish\",\n",
    "        ),\n",
    "        french_agent.as_tool(\n",
    "            tool_name=\"translate_to_french\",\n",
    "            tool_description=\"Translate the user's message to French\",\n",
    "        ),\n",
    "        italian_agent.as_tool(\n",
    "            tool_name=\"translate_to_italian\",\n",
    "            tool_description=\"Translate the user's message to Italian\",\n",
    "        ),\n",
    "    ],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Final processing agent\n",
    "synthesizer_agent = Agent(\n",
    "    name=\"synthesizer_agent\",\n",
    "    instructions=\"You inspect translations, correct them if needed, and produce a final concatenated response.\",\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Async function for notebook\n",
    "async def run_translation_toolflow():\n",
    "    msg = input(\"Hi! What would you like translated, and to which languages? \")\n",
    "\n",
    "    with trace(\"Orchestrator evaluator\"):\n",
    "        # First, orchestration with tool usage\n",
    "        print(\"\\n🔧 Orchestrator is running with selected tools...\\n\")\n",
    "        orchestrator_result = await Runner.run(orchestrator_agent, msg)\n",
    "\n",
    "        for item in orchestrator_result.new_items:\n",
    "            if isinstance(item, MessageOutputItem):\n",
    "                text = ItemHelpers.text_message_output(item)\n",
    "                if text:\n",
    "                    print(f\"🧩 Translation step:\\n{text}\\n\")\n",
    "\n",
    "        # Then pass everything to the synthesizer\n",
    "        print(\"🧪 Synthesizer is generating final output...\\n\")\n",
    "        synthesizer_result = await Runner.run(\n",
    "            synthesizer_agent, orchestrator_result.to_input_list()\n",
    "        )\n",
    "\n",
    "    print(f\"\\n✅ Final response:\\n{synthesizer_result.final_output}\")\n",
    "\n",
    "# Run in notebook\n",
    "await run_translation_toolflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example shows the LLM as a judge pattern. The first agent generates an outline for a story.\n",
    "The second agent judges the outline and provides feedback. We loop until the judge is satisfied\n",
    "with the outline.\n",
    "\"\"\"\n",
    "\n",
    "story_outline_generator = Agent(\n",
    "    name=\"story_outline_generator\",\n",
    "    instructions=(\n",
    "        \"You generate a very short story outline based on the user's input.\"\n",
    "        \"If there is any feedback provided, use it to improve the outline.\"\n",
    "    ),\n",
    "    model=model\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class EvaluationFeedback:\n",
    "    feedback: str\n",
    "    score: Literal[\"pass\", \"needs_improvement\", \"fail\"]\n",
    "\n",
    "evaluator = Agent[None](\n",
    "    name=\"evaluator\",\n",
    "    instructions=(\n",
    "        \"You evaluate a story outline and decide if it's good enough.\"\n",
    "        \"If it's not good enough, you provide feedback on what needs to be improved.\"\n",
    "        \"Never give it a pass on the first try.\"\n",
    "    ),\n",
    "    output_type=EvaluationFeedback,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "async def main() -> None:\n",
    "    msg = input(\"What kind of story would you like to hear? \")\n",
    "    input_items: list[TResponseInputItem] = [{\"content\": msg, \"role\": \"user\"}]\n",
    "\n",
    "    latest_outline: str | None = None\n",
    "\n",
    "    # We'll run the entire workflow in a single trace\n",
    "    with trace(\"LLM as a judge\"):\n",
    "        while True:\n",
    "            story_outline_result = await Runner.run(\n",
    "                story_outline_generator,\n",
    "                input_items,\n",
    "            )\n",
    "\n",
    "            input_items = story_outline_result.to_input_list()\n",
    "            latest_outline = ItemHelpers.text_message_outputs(story_outline_result.new_items)\n",
    "            print(\"Story outline generated\")\n",
    "\n",
    "            evaluator_result = await Runner.run(evaluator, input_items)\n",
    "            result: EvaluationFeedback = evaluator_result.final_output\n",
    "\n",
    "            print(f\"Evaluator score: {result.score}\")\n",
    "\n",
    "            if result.score == \"pass\":\n",
    "                print(\"Story outline is good enough, exiting.\")\n",
    "                break\n",
    "\n",
    "            print(\"Re-running with feedback\")\n",
    "\n",
    "            input_items.append({\"content\": f\"Feedback: {result.feedback}\", \"role\": \"user\"})\n",
    "\n",
    "    print(f\"Final story outline: {latest_outline}\")\n",
    "\n",
    "# Run the async function in Jupyter\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example shows the parallelization pattern. We run the agent three times in parallel, and pick\n",
    "the best result.\n",
    "\"\"\"\n",
    "\n",
    "# Add your model reference before running, e.g.:\n",
    "# model = your_model_instance\n",
    "\n",
    "spanish_agent = Agent(\n",
    "    name=\"spanish_agent\",\n",
    "    instructions=\"You translate the user's message to Spanish\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "translation_picker = Agent(\n",
    "    name=\"translation_picker\",\n",
    "    instructions=\"You pick the best Spanish translation from the given options.\",\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    msg = input(\"Hi! Enter a message, and we'll translate it to Spanish.\\n\\n\")\n",
    "\n",
    "    # Ensure the entire workflow is a single trace\n",
    "    with trace(\"Parallel translation\"):\n",
    "        res_1, res_2, res_3 = await asyncio.gather(\n",
    "            Runner.run(spanish_agent, msg),\n",
    "            Runner.run(spanish_agent, msg),\n",
    "            Runner.run(spanish_agent, msg),\n",
    "        )\n",
    "\n",
    "        outputs = [\n",
    "            ItemHelpers.text_message_outputs(res_1.new_items),\n",
    "            ItemHelpers.text_message_outputs(res_2.new_items),\n",
    "            ItemHelpers.text_message_outputs(res_3.new_items),\n",
    "        ]\n",
    "\n",
    "        translations = \"\\n\\n\".join(outputs)\n",
    "        print(f\"\\n\\nTranslations:\\n\\n{translations}\")\n",
    "\n",
    "        best_translation = await Runner.run(\n",
    "            translation_picker,\n",
    "            f\"Input: {msg}\\n\\nTranslations:\\n{translations}\",\n",
    "        )\n",
    "\n",
    "    print(\"\\n\\n-----\")\n",
    "    print(f\"Best translation: {best_translation.final_output}\")\n",
    "\n",
    "# Run the async function in Jupyter\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This example shows how to use guardrails.\n",
    "\n",
    "Guardrails are checks that run in parallel to the agent's execution.\n",
    "They can be used to do things like:\n",
    "- Check if input messages are off-topic\n",
    "- Check that output messages don't violate any policies\n",
    "- Take over control of the agent's execution if an unexpected input is detected\n",
    "\n",
    "In this example, we'll setup an input guardrail that trips if the user is asking to do math homework.\n",
    "If the guardrail trips, we'll respond with a refusal message.\n",
    "\"\"\"\n",
    "\n",
    "# Define your model before running this cell, e.g.:\n",
    "# model = your_model_instance\n",
    "\n",
    "### 1. An agent-based guardrail that is triggered if the user is asking to do math homework\n",
    "class MathHomeworkOutput(BaseModel):\n",
    "    reasoning: str\n",
    "    is_math_homework: bool\n",
    "\n",
    "guardrail_agent = Agent(\n",
    "    name=\"Guardrail check\",\n",
    "    instructions=\"Check if the user is asking you to do their math homework.\",\n",
    "    output_type=MathHomeworkOutput,\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "@input_guardrail\n",
    "async def math_guardrail(\n",
    "    context: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]\n",
    ") -> GuardrailFunctionOutput:\n",
    "    \"\"\"This is an input guardrail function, which happens to call an agent to check if the input\n",
    "    is a math homework question.\n",
    "    \"\"\"\n",
    "    result = await Runner.run(guardrail_agent, input, context=context.context)\n",
    "    final_output = result.final_output_as(MathHomeworkOutput)\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info=final_output,\n",
    "        tripwire_triggered=final_output.is_math_homework,\n",
    "    )\n",
    "\n",
    "### 2. The run loop\n",
    "async def main():\n",
    "    agent = Agent(\n",
    "        name=\"Customer support agent\",\n",
    "        instructions=\"You are a customer support agent. You help customers with their questions.\",\n",
    "        input_guardrails=[math_guardrail],\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    input_data: list[TResponseInputItem] = []\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"Enter a message: \")\n",
    "        input_data.append(\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = await Runner.run(agent, input_data)\n",
    "            print(result.final_output)\n",
    "            # If the guardrail didn't trigger, we use the result as the input for the next run\n",
    "            input_data = result.to_input_list()\n",
    "        except InputGuardrailTripwireTriggered:\n",
    "            # If the guardrail triggered, we instead add a refusal message to the input\n",
    "            message = \"Sorry, I can't help you with your math homework.\"\n",
    "            print(message)\n",
    "            input_data.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": message,\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Run the async function in Jupyter\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from agents import (\n",
    "    Agent,\n",
    "    GuardrailFunctionOutput,\n",
    "    OutputGuardrailTripwireTriggered,\n",
    "    RunContextWrapper,\n",
    "    Runner,\n",
    "    output_guardrail,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "This example shows how to use output guardrails.\n",
    "\n",
    "Output guardrails are checks that run on the final output of an agent.\n",
    "They can be used to do things like:\n",
    "- Check if the output contains sensitive data\n",
    "- Check if the output is a valid response to the user's message\n",
    "\n",
    "In this example, we'll use a (contrived) example where we check if the agent's response contains\n",
    "a phone number.\n",
    "\"\"\"\n",
    "\n",
    "# Make sure to define your model beforehand:\n",
    "# model = your_model_instance\n",
    "\n",
    "# The agent's output type\n",
    "class MessageOutput(BaseModel):\n",
    "    reasoning: str = Field(description=\"Thoughts on how to respond to the user's message\")\n",
    "    response: str = Field(description=\"The response to the user's message\")\n",
    "    user_name: str | None = Field(description=\"The name of the user who sent the message, if known\")\n",
    "\n",
    "\n",
    "@output_guardrail\n",
    "async def sensitive_data_check(\n",
    "    context: RunContextWrapper, agent: Agent, output: MessageOutput\n",
    ") -> GuardrailFunctionOutput:\n",
    "    phone_number_in_response = \"650\" in output.response\n",
    "    phone_number_in_reasoning = \"650\" in output.reasoning\n",
    "\n",
    "    return GuardrailFunctionOutput(\n",
    "        output_info={\n",
    "            \"phone_number_in_response\": phone_number_in_response,\n",
    "            \"phone_number_in_reasoning\": phone_number_in_reasoning,\n",
    "        },\n",
    "        tripwire_triggered=phone_number_in_response or phone_number_in_reasoning,\n",
    "    )\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"Assistant\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    output_type=MessageOutput,\n",
    "    output_guardrails=[sensitive_data_check],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# Main logic for testing the guardrail behavior\n",
    "async def main():\n",
    "    # This should be ok\n",
    "    await Runner.run(agent, \"What's the capital of California?\")\n",
    "    print(\"First message passed\")\n",
    "\n",
    "    # This should trip the guardrail\n",
    "    try:\n",
    "        result = await Runner.run(\n",
    "            agent, \"My phone number is 650-123-4567. Where do you think I live?\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Guardrail didn't trip - this is unexpected. Output: {json.dumps(result.final_output.model_dump(), indent=2)}\"\n",
    "        )\n",
    "\n",
    "    except OutputGuardrailTripwireTriggered as e:\n",
    "        print(f\"Guardrail tripped. Info: {e.guardrail_result.output.output_info}\")\n",
    "\n",
    "# Run the async function in Jupyter\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aiworkshop)",
   "language": "python",
   "name": "aiworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
