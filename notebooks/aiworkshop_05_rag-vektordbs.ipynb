{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.5: Vektordatenbanken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- source: https://milvus.io/docs/de/v2.4.x/build_RAG_with_milvus_and_ollama.md\n",
    "    - milvus lite: https://github.com/milvus-io/milvus-lite\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiworkshop_utils.standardlib_imports import os, json, base64, logging, Optional, Any, Dict, List, pprint, shutil, glob\n",
    "from aiworkshop_utils.thirdparty_imports import AutoTokenizer, load_dotenv, requests, BaseModel, Field, pd, cosine_similarity, plt, np, DataType, MilvusClient, tqdm, SentenceTransformer\n",
    "from aiworkshop_utils.custom_utils import show_pretty_json, encode_image, load_md_sections_from_path\n",
    "from aiworkshop_utils.jupyter_imports import Markdown, HTML, widgets, display, JSON\n",
    "from aiworkshop_utils.docling_imports import PyPdfiumDocumentBackend, HybridChunker, InputFormat, AcceleratorDevice, AcceleratorOptions, PdfPipelineOptions, DocumentConverter, PdfFormatOption, WordFormatOption, SimplePipeline\n",
    "from aiworkshop_utils.openai_imports import openaisdk_client_chat_completions_create\n",
    "from aiworkshop_utils import config\n",
    "from aiworkshop_utils import embedders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Konzepte: Vektordatenbanken und RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- milvus\n",
    "    - Info: f√ºr kleinere Datens√§tze mit bis zu einigen Millionen Vektoren -> Milvus Lite ist ok\n",
    "    - f√ºr Datens√§tze bis zu 100 Millionen Vektoren -> besser Milvus Standalone\n",
    "\n",
    "- ollama\n",
    "    - Info: oft vorteilhaft, spezialisierte Modelle f√ºr Embedding-Aufgaben einzusetzen\n",
    "    - kann Leistung in Aufgaben wie semantischer Suche verbessern, sind auch ressourcenschonender, effizienter\n",
    "\n",
    "- Muss das Modell, das Embeddings vornimmt, das gleiche sein, wie das Generierungsmodell einer Chat-Anwendung?\n",
    "    - Das Embedding Modell muss nicht das Generierungs-Modell sein, das sp√§ter ben√ºtzt wird bzw. muss es nicht von der gleichen Modell-Familie sein in RAG Prozesen -> Solange das gleiche Embedding Modell f√ºr alle Embeddings benutzt wird, ist alles gut\n",
    "        - Prompt wird mit Embedding Modell eingebettet, dann findet eine semantische Suche in einer Vektordatenbank statt, Chunks der eingebetteten Dokumente kommen zur√ºck\n",
    "        - Generierungsmodell ben√ºtzt den Prompt und die gefundenen Chunks und gibt dies in den Kontext -> es gibt hier keine √úberschneidug mit dem Embedding-Vektorraum\n",
    "        - Quelle: https://ericmjl.github.io/blog/2024/1/15/your-embedding-model-can-be-different-from-your-text-generation-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Milvus Beispiel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Verbindung zu Milvus Lite herstellen und Collection erstellen\n",
    "# -------------------------------\n",
    "\n",
    "# Verbindung zum Milvus-Server herstellen\n",
    "milvus_client = MilvusClient(config.MILVUS_DB)\n",
    "\n",
    "# Collection-Name und Vektordimension definieren\n",
    "collection_name = config.MILVUS_COLLECTION_A\n",
    "embedding_dims = 768 # NOMIC-Model Default\n",
    "\n",
    "# √úberpr√ºfen, ob die Collection bereits existiert, und ggf. l√∂schen\n",
    "if milvus_client.has_collection(collection_name=collection_name):\n",
    "    milvus_client.drop_collection(collection_name=collection_name)\n",
    "\n",
    "# Collection erstellen\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedding_dims,\n",
    "    primary_field_name='id',\n",
    "    id_type=DataType.INT64,\n",
    "    vector_field_name='vector',\n",
    "    metric_type='IP',  # Inner Product Distance\n",
    "    auto_id=True, # erstellt automatisch IDs\n",
    "    consistency_level='Strong',\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' mit Dimension {embedding_dims} erstellt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Beispieltexte definieren und Embeddings erstellen\n",
    "# -------------------------------\n",
    "\n",
    "# Beispiel-Dokumente\n",
    "docs = [\n",
    "    \"Artificial intelligence was founded as an academic discipline in 1956.\",\n",
    "    \"Alan Turing was the first person to conduct substantial research in AI.\",\n",
    "    \"Born in Maida Vale, London, Turing was raised in southern England.\",\n",
    "    \"In 2022, OpenAI made ChatGPT publicly available.\",\n",
    "]\n",
    "subject = \"history\"\n",
    "\n",
    "milvusbsp1_embeddings = embedder_ollama.get_embeddings(docs)\n",
    "\n",
    "print(f\"Embeddings f√ºr alle Dokumente abgerufen. Anzahl: {len(milvusbsp1_embeddings)}\")\n",
    "show_pretty_json(milvusbsp1_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. Daten in Milvus Lite einf√ºgen und Suche durchf√ºhren\n",
    "# -------------------------------\n",
    "\n",
    "# Daten vorbereiten ‚Äì jeder Eintrag enth√§lt eine eindeutige ID, den Vektor, den Text und ein zus√§tzliches Feld \"subject\"\n",
    "data = [\n",
    "    {\"id\": i, \"vector\": milvusbsp1_embeddings[i], \"text\": docs[i], \"subject\": subject}\n",
    "    for i in range(len(docs))\n",
    "]\n",
    "\n",
    "insert_res = milvus_client.insert(collection_name=collection_name, data=data)\n",
    "print(\"Insert result:\", insert_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 4. Suche: Verwende z.B. das Embedding des ersten Dokuments als Suchvektor\n",
    "# -------------------------------\n",
    "query_vector = milvusbsp1_embeddings[0]\n",
    "search_res = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[query_vector],\n",
    "    filter=\"subject == 'history'\",\n",
    "    limit=2,\n",
    "    output_fields=[\"text\", \"subject\"]\n",
    ")\n",
    "\n",
    "print(\"Search result:\")\n",
    "show_pretty_json(search_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Beispiel-Prompt an das Conversational Model (llama3.1) senden\n",
    "# -------------------------------\n",
    "# Baue einen Prompt mit den Suchergebnissen als Kontext.\n",
    "# Hier werden alle gefundenen Texte zusammengef√ºhrt.\n",
    "# F√ºr eine einzelne Abfrage: Greife auf die Trefferliste der ersten Query zu.\n",
    "hits = search_res[0]  # Direkt verwenden, da hits bereits eine Liste von Dictionaries ist\n",
    "context_texts = \" \".join([hit[\"entity\"][\"text\"] for hit in hits])\n",
    "\n",
    "print(\"Context texts:\")\n",
    "show_pretty_json(context_texts)\n",
    "\n",
    "conversation_prompt = (\n",
    "    f\"Basierend auf den folgenden Informationen: {context_texts}\\n\"\n",
    "    \"Benutze nur diese Informationen, um die Frage zu beantworten.\\n\"\n",
    "    \"Beantworte: Was sind wichtige Meilensteine in der Geschichte der k√ºnstlichen Intelligenz?\"\n",
    ")\n",
    "\n",
    "print(\"Conversation prompt:\")\n",
    "show_pretty_json(conversation_prompt)\n",
    "\n",
    "# Sende den Request an den Conversational-Endpoint\n",
    "conv_response = requests.post(\n",
    "    config.OAPI_GENERATE_URL,\n",
    "    json={\"model\": config.OMODEL_MISTRALNEMO, \"prompt\": conversation_prompt}\n",
    ")\n",
    "\n",
    "# Lies den kompletten rohen Response-Text aus und teile ihn in einzelne Zeilen\n",
    "raw_response = conv_response.text.strip()\n",
    "lines = raw_response.splitlines()\n",
    "\n",
    "# Fasse die 'response'-Teile aus jeder Zeile zusammen\n",
    "full_response = \"\"\n",
    "for line in lines:\n",
    "    try:\n",
    "        json_obj = json.loads(line)\n",
    "        full_response += json_obj.get(\"response\", \"\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Fehler beim Parsen der Zeile:\", line, e)\n",
    "\n",
    "print(\"Full conversation model output:\")\n",
    "print(full_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √úberpr√ºfen, ob die Collection bereits existiert, und ggf. l√∂schen\n",
    "if milvus_client.has_collection(collection_name=collection_name):\n",
    "    milvus_client.drop_collection(collection_name=collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Milvus Beispiel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 1. Verbindung zu Milvus Lite herstellen und Collection erstellen\n",
    "# -------------------------------\n",
    "\n",
    "# Verbindung zum Milvus-Server herstellen\n",
    "milvus_client = MilvusClient(config.MILVUS_DB)\n",
    "\n",
    "# Collection-Name und Vektordimension definieren\n",
    "collection_name = config.MILVUS_COLLECTION_A\n",
    "embedding_dims = 768 # NOMIC-Model Default\n",
    "\n",
    "# √úberpr√ºfen, ob die Collection bereits existiert, und ggf. l√∂schen\n",
    "if milvus_client.has_collection(collection_name=collection_name):\n",
    "    milvus_client.drop_collection(collection_name=collection_name)\n",
    "\n",
    "# Collection erstellen\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedding_dims,\n",
    "    primary_field_name='id',\n",
    "    id_type=DataType.INT64,\n",
    "    vector_field_name='vector',\n",
    "    metric_type='IP',  # Inner Product Distance\n",
    "    auto_id=True, # erstellt automatisch IDs\n",
    "    consistency_level='Strong',\n",
    ")\n",
    "\n",
    "print(f\"Collection '{collection_name}' mit Dimension {embedding_dims} erstellt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 2. Beispieltexte definieren und Embeddings erstellen\n",
    "# -------------------------------\n",
    "\n",
    "example_text_lines = load_md_sections_from_path(\"milvus_docs/en/faq/*.md\") # Chunking eines MD-Files!\n",
    "show_pretty_json(example_text_lines)\n",
    "\n",
    "data = []\n",
    "for i, line in enumerate(tqdm(example_text_lines, desc=\"Creating embeddings\")):\n",
    "    data.append(\n",
    "        {\n",
    "            \"id\": i, \n",
    "            \"vector\": embedder_ollama.get_embeddings(line), \n",
    "            \"text\": line\n",
    "            }\n",
    "            )\n",
    "\n",
    "print(len(data))\n",
    "show_pretty_json(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. A) Daten in Milvus Lite einf√ºgen\n",
    "# -------------------------------\n",
    "milvus_client.insert(\n",
    "    collection_name=config.MILVUS_COLLECTION_A, \n",
    "    data=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = milvus_client.get_collection_stats(config.MILVUS_COLLECTION_A)\n",
    "print(stats)\n",
    "print(stats['row_count']*embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Disk Space Information ===\n",
    "# Update this to match your Milvus data directory as configured in milvus.yaml\n",
    "milvus_data_path = \"milvus_db.db\"\n",
    "total, used, free = shutil.disk_usage(milvus_data_path)\n",
    "\n",
    "print(\"Disk Space Info:\")\n",
    "print(\"  Total: {:.2f} GB\".format(total / 2**30))\n",
    "print(\"  Used:  {:.2f} GB\".format(used / 2**30))\n",
    "print(\"  Free:  {:.2f} GB\".format(free / 2**30))\n",
    "print(\"\")\n",
    "\n",
    "# === Milvus Collection Stats ===\n",
    "stats = milvus_client.get_collection_stats(config.MILVUS_COLLECTION_A)\n",
    "current_vectors = stats.get('row_count', 0)\n",
    "print(\"Milvus Collection Stats:\")\n",
    "print(\"  Current number of vectors: {}\".format(current_vectors))\n",
    "\n",
    "# === Estimating Storage Usage per Vector ===\n",
    "# Update embedding_dims to your actual vector dimension.\n",
    "embedding_dims = 512  \n",
    "# Assuming float32 (4 bytes per dimension)\n",
    "bytes_per_vector = embedding_dims * 4  \n",
    "print(\"Approximate storage used by one vector:\")\n",
    "print(\"  {} bytes ({:.2f} KB)\".format(bytes_per_vector, bytes_per_vector / 1024))\n",
    "print(\"\")\n",
    "\n",
    "# === Estimating Total Vector Storage Usage ===\n",
    "total_vector_bytes = current_vectors * bytes_per_vector\n",
    "print(\"Approximate storage used by vectors: {:.2f} MB\".format(total_vector_bytes / (1024**2)))\n",
    "print(\"\")\n",
    "\n",
    "# === Estimating Remaining Capacity ===\n",
    "# Adjust max_vectors_estimate based on your deployment expectations.\n",
    "max_vectors_estimate = 2_000_000  \n",
    "remaining_vectors = max_vectors_estimate - current_vectors\n",
    "print(\"Estimated remaining capacity (vector count): {:,}\".format(remaining_vectors).replace(\",\", \".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 3. B) Suche durchf√ºhren\n",
    "# -------------------------------\n",
    "question = \"How is data stored in milvus?\"\n",
    "\n",
    "search_res = milvus_client.search(\n",
    "    collection_name=config.MILVUS_COLLECTION_A,\n",
    "    data=[embedder_ollama.get_embeddings(question)],  # embed the prompt!\n",
    "    limit=3,  # Return top 3 results\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "    output_fields=[\"text\"],  # Return the text field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# 5. Beispiel-Prompt an das Conversational Model (llama3.1) senden\n",
    "# -------------------------------\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "\"\"\"\n",
    "USER_PROMPT = f\"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format request as per Ollama API\n",
    "data = {\n",
    "    \"model\": config.OMODEL_MISTRALNEMO,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "# Send the request\n",
    "response = requests.post(config.OAPI_CHAT_URL, json=data)\n",
    "response_json = response.json()\n",
    "\n",
    "# Debugging: Print full response\n",
    "print(\"Full API Response:\", response_json)\n",
    "\n",
    "# Extract message content\n",
    "#print(response_json[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the assistant's message content\n",
    "assistant_message = response_json.get(\"message\", {}).get(\"content\", \"No response received.\")\n",
    "\n",
    "# Print it in a user-friendly format\n",
    "print(\"\\nü§ñ **Assistant's Response:**\\n\")\n",
    "print(assistant_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[√úBUNG_2.5.01]** Docling-Milvus-Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Diese √úbung ist Teil von √úBUNG5. Speichere deine Ergebnisse und Notizen in einem File (Word), f√ºge dann noch die restlichen √úbungen dieser Session 2.4 hinzu und lade sie auf Moodle unter \"Abgabe √úbung 5\" bis zum 20.05.25 hoch.*\n",
    "\n",
    "- Bastle eine kleine Pipeline f√ºr [Parsing -> Chunking -> Embedding -> Indexing]\n",
    "- F√ºhre dies mit einem PDF-Dokument deiner Wahl durch\n",
    "- Du kanst f√ºr Parsing und Chunking Docling ben√ºtzen\n",
    "- F√ºr Embedding kannst du ein Ollama-Model verwenden\n",
    "- Erstelle f√ºr Indexing eine Milvus Lite DB Instanz und f√ºge dort die Vektoren der Chunks ein"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
