{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.2: LLM-API-Interaktion, Kontext und Konversations-Historie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- OpenAI Developer Platform: https://platform.openai.com/docs/overview\n",
    "- OpenAI API reference: https://platform.openai.com/docs/api-reference/introduction\n",
    "- Ollama API Docs: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "- Ollama Model File: https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "- Temperature und Token-Probability: https://artefact2.github.io/llm-sampling/index.xhtml\n",
    "- Explained: Tokens and Embeddings in LLMs (Medium) https://medium.com/the-research-nest/explained-tokens-and-embeddings-in-llms-69a16ba5db33\n",
    "- Meta/Llama auf Huggingface: https://huggingface.co/meta-llama\n",
    "- Deepseek Platform: https://platform.deepseek.com/usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T09:29:46.082725Z",
     "start_time": "2025-04-05T09:29:45.961445Z"
    }
   },
   "outputs": [],
   "source": [
    "from aiworkshop_utils.standardlib_imports import os, json, base64, logging, Optional, List\n",
    "from aiworkshop_utils.thirdparty_imports import AutoTokenizer, load_dotenv, requests, BaseModel, Field\n",
    "from aiworkshop_utils.custom_utils import show_pretty_json, encode_image\n",
    "from aiworkshop_utils.jupyter_imports import Markdown, HTML, widgets\n",
    "from aiworkshop_utils.openai_imports import OpenAI\n",
    "from aiworkshop_utils import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Konzept: LLM-APIs und -SDKs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vor-/Nachteile, LLMs √ºber Cloud oder on Premise ansprechen\n",
    "- Firmen bieten LLM-Serving √ºber APIs an - mit API-key ansprechbar, Verrechnung nach Tokens\n",
    "- OpenAI API und SDK ist der Standard, weil sie die First-Mover sind\n",
    "- Vorsicht: Verwechselung bei der Aussprache \"OpenAI API\" und \"Open API\" (Swagger)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Screenshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beispiel: Deepseek-Plattform Usage Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deepseek Usage](assets/image01_deepseek-platform-usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beispiel: Deepseek Models & Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deepseek Platform Pricing](assets/image02_deepseek-platform-pricing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beispiel: OpenAI Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Deepseek Platform Pricing](assets/image03_openai-platform-pricing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Live-Beispiel Deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "client = OpenAI(api_key=config.DEEPSEEK_API_KEY, base_url=config.DEEPSEEK_API_URL)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Wie hat sich die Einwohnerzahl in √ñsterreich entwickelt?\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° OpenAI-SDK-Client erstellen mit lokaler Ollama API Base URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openaisdk_client = OpenAI(\n",
    "    base_url=config.OAPI_OPENAISDK_BASE_URL,\n",
    "    api_key=config.OLLAMA_FAKE_API_KEY,  # fake key!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Erster Test einer lokalen OpenAI SDK Chat Completion mit Ollama API im Hintergrund"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Temperature und Token-Probability: https://artefact2.github.io/llm-sampling/index.xhtml\n",
    "- neue API: Responses (https://platform.openai.com/docs/guides/text?api-mode=responses)\n",
    "- alte API: Chat Completions (https://platform.openai.com/docs/guides/text?api-mode=chat)\n",
    "- -> Ollama hat Responses noch nicht!\n",
    "- Create chat completions Parameter: https://platform.openai.com/docs/api-reference/chat/create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"id\": \"chatcmpl-556\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"message\": {\n",
       "        \"content\": \"Here's one:\\n\\nWhy do programmers prefer dark mode?\\n\\nBecause light attracts bugs.\",\n",
       "        \"refusal\": null,\n",
       "        \"role\": \"assistant\",\n",
       "        \"annotations\": null,\n",
       "        \"audio\": null,\n",
       "        \"function_call\": null,\n",
       "        \"tool_calls\": null\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1743886046,\n",
       "  \"model\": \"llama3.2:latest\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"service_tier\": null,\n",
       "  \"system_fingerprint\": \"fp_ollama\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 17,\n",
       "    \"prompt_tokens\": 39,\n",
       "    \"total_tokens\": 56,\n",
       "    \"completion_tokens_details\": null,\n",
       "    \"prompt_tokens_details\": null\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openaisdk_client.chat.completions.create(\n",
    "  model=config.OMODEL_LLAMA3D2, #config.OMODEL_DEEPSEEK\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about a programmer.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    "  top_p=1, # zwischen 0 und 1, bestimmt, aus welchen Token das Modell beim n√§chsten Schritt w√§hlt; kumulative Wahrscheinlichkeit >= top_p\n",
    "  stream=False,\n",
    "  #max_tokens=10,\n",
    "  #stop=None,\n",
    "  #presence_penalty=0,\n",
    "  #frequency_penalty=0,\n",
    "  #n=3, -> funktioniert nicht in Ollama\n",
    "  #logprobs=5 -> funktioniert nicht in Ollama\n",
    ")\n",
    "\n",
    "show_pretty_json(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[√úBUNG_2.2.01]** Teste Single Completions mit unterschiedlichen Parametern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Diese √úbung ist Teil von √úBUNG3. Speichere deine Ergebnisse und Notizen in einem File (Word), f√ºge dann noch die restlichen √úbungen dieser Session 2.2 hinzu und lade sie auf Moodle unter \"Abgabe √úbung 3\" bis zum 20.05.25 hoch.*\n",
    "\n",
    "- TODOs:\n",
    "    - Wechsle das Model aus\n",
    "    - Probiere verschiedene User-Prompts aus\n",
    "    - Probiere verschiedene Werte f√ºr die Parameter, speziell f√ºr Temperatur\n",
    "- Fragen:\n",
    "    - Was bedeuten die einzelnen Keys im Request und in der Response? (Tipp: https://platform.openai.com/docs/api-reference/chat/create) (muss nicht dokumentiert werden)\n",
    "    - Welche Erfahrungen machst du?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Mehr Low-Level-Infos durch Interaktion mit Ollama-API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ollama Endpunkte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---- COMPLETION ----\n",
    "    A) Generate a completion       POST /api/generate\n",
    "    B) Generate a chat completion  POST /api/chat\n",
    "\n",
    "---- MODELS ----\n",
    "    C) Create a Model              POST /api/create\n",
    "    D) List Local Models           GET /api/tags\n",
    "    E) Show Model Information      POST /api/show\n",
    "    F) Copy a Model                POST /api/copy\n",
    "    G) Delete a Model              DELETE /api/delete\n",
    "    H) Pull a Model                POST /api/pull\n",
    "    I) Push a Model                POST /api/push\n",
    "\n",
    "---- EMBEDDINGS ----\n",
    "    J) Generate Embeddings         POST /api/embed\n",
    "\n",
    "---- ELSE ----\n",
    "    K) List Running Models         GET /api/ps\n",
    "    L) Version                     GET /api/version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ‚ö° Ollama Endpunkt (K) 'List Running Models' GET /api/ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434/api/tags\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(json.dumps(data, indent=4))\n",
    "else:\n",
    "    print(\"Request failed with status code\", response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\"name\": \"nomic-embed-text:latest\",\n",
    "\"model\": \"nomic-embed-text:latest\",\n",
    "\"modified_at\": \"2025-03-29T15:31:2...\",\n",
    "\"size\": 274302450,                           # in Bytes angegeben (ca. 2.02GB)\n",
    "\"digest\": \"0a109f422b47e3a......\",           # digitaler Fingerabdruck\n",
    "\"details\":\n",
    "    \"parent_model\": \"\",\n",
    "    \"format\": \"gguf\",                        # GGUF: \"Grokking GGML Unified Format\", optimiert f√ºr lokale Ausf√ºhrung (ggml = leichtgw. ML-Lib)\n",
    "    \"family\": \"nomic-bert\",\n",
    "    \"families\": [\n",
    "        \"nomic-bert\"\n",
    "    ],\n",
    "    \"parameter_size\": \"137M\",                # 137 Millionen Parameter (Weights and Biases)\n",
    "    \"quantization_level\": \"F16\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![√úbersicht KI-Modell-Formate](assets/image05_√ºbersicht-ki-modell-formate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![√úbersicht Quantisierungsstufen](assets/image06_quantisierungsstufen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Ollama Endpunkt (A) 'Generate a completion' POST /api/generate: Einfache Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter**\n",
    "- `model`: *(erforderlich)* Name des Modells  \n",
    "- `prompt`: Eingabeaufforderung f√ºr die Generierung einer Antwort  \n",
    "- `suffix`: Text, der nach der Modellantwort eingef√ºgt wird (nicht bei llama3.1 -> LLaMA-based models (like llama3) are designed for sequential text generation rather than filling in missing text between a prompt and suffix.)\n",
    "- `images` *(optional)*: Liste von base64-codierten Bildern (f√ºr multimodale Modelle wie LLaVA)  \n",
    "\n",
    "**Erweiterte Parameter**\n",
    "- `format`: Format der Antwort *(json oder JSON-Schema)*  \n",
    "- `options`: Zus√§tzliche Modellparameter laut Modelfile-Dokumentation *(z. B. Temperatur)*  \n",
    "- `system`: Systemnachricht *(√ºberschreibt die im Modelfile definierte Nachricht)*  \n",
    "- `template`: Vorlage f√ºr den Prompt *(√ºberschreibt die im Modelfile definierte Vorlage)*  \n",
    "- `stream`:  \n",
    "  - **false**: Die Antwort wird als einzelnes Objekt zur√ºckgegeben  \n",
    "  - **true**: Antwort erfolgt als Stream  \n",
    "- `raw`:  \n",
    "  - **true**: Kein Formatierungsaufwand, n√ºtzlich f√ºr vollst√§ndig vorformulierte Prompts  \n",
    "  - **false**: Standardformatierung wird angewendet  \n",
    "- `keep_alive`:  \n",
    "  - Zeitspanne, in der das Modell nach der Anfrage im Speicher bleibt *(Standard: 5 Minuten)*  \n",
    "- `context` *(veraltet)*:  \n",
    "  - Verwendet zur Aufrechterhaltung kurzer Konversationskontexte aus vorherigen Anfragen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# vars\n",
    "# --------------\n",
    "endpoint = config.OAPI_GENERATE_URL\n",
    "model = config.OMODEL_LLAMA3D2\n",
    "#prompt = \"Give me one random word\"\n",
    "#prompt = '''Here is a question and an answer:\n",
    "#    Question: How old are you?\n",
    "#    Answer: I live in California.\n",
    "#    Does the answer seem valid? Reply with 'Yes' or 'No' and explain briefly.'''\n",
    "prompt = \"Hi!\"\n",
    "stream_bool = False\n",
    "\n",
    "# --------------\n",
    "# execution\n",
    "# --------------\n",
    "# Request Data:\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": stream_bool\n",
    "}\n",
    "# Send Request and Jsonify it, to be able to print it:\n",
    "response = requests.post(endpoint, json=data)\n",
    "response_json = response.json()\n",
    "response_json_indented = json.dumps(response_json, indent=4, ensure_ascii=False)\n",
    "response_text = response_json[\"response\"]\n",
    "tokens_per_second = response_json[\"eval_count\"] / response_json[\"eval_duration\"] * 10**9\n",
    "\n",
    "# --------------\n",
    "# output\n",
    "# --------------\n",
    "print(response_json_indented)\n",
    "print(\"##### response #####\")\n",
    "print (response_text)\n",
    "print(\" \")\n",
    "print(\"##### tokens per second #####\")\n",
    "print(tokens_per_second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_json['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[√úBUNG_2.2.02]** Welchen Text ergeben diese Token IDs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Diese √úbung ist Teil von √úBUNG3. Speichere deine Ergebnisse und Notizen in einem File (Word), f√ºge dann noch die restlichen √úbungen dieser Session 2.2 hinzu und lade sie auf Moodle unter \"Abgabe √úbung 3\" bis zum 20.05.25 hoch.*\n",
    "- Token-IDs:\n",
    "    - [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 19841, 0, 8574, 9857, 12, 31566, 48750, 304, 2991, 436, 19919, 74, 347, 17465, 0, 37674, 574, 82931, 295, 6754, 364, 68, 354, 6, 304, 364, 128009, 71090, 4060, 315, 12268, 304, 17495, 24277, 23124, 30, 128009, 128006, 78191, 128007, 271, 53545, 11, 10864, 9736, 13091, 71, 11, 15297, 3930, 2815, 9857, 12, 31566, 27348, 306, 34525, 83, 34143, 2268, 33717, 330, 68, 354, 1, 737, 2991, 55483, 7328, 330, 3812, 315, 12268, 498, 1101, 2761, 22855, 94483, 17495, 24277, 74095, 81, 29965, 13, 9419, 6127, 4466, 2395, 8510, 37907, 9857, 11, 6754, 75291, 15165, 11, 4543, 78968, 799, 13462, 11, 15297, 10021, 24277, 23124, 387, 55181, 6127, 2073, 32457, 31732, 268, 96917, 12666, 71877, 2781, 36708, 295, 12931, 382, 644, 89787, 13149, 12, 2073, 15883, 613, 424, 82212, 784, 615, 268, 15165, 47768, 9857, 75291, 11, 4543, 3453, 11246, 21740, 56013, 2761, 24277, 23124, 6529, 75361, 51691, 12778, 13, 9419, 6127, 13672, 4466, 330, 3812, 68, 1, 951, 21031, 652, 32673, 82, 11, 13582, 2486, 98244, 3744, 68, 3675, 76230, 11, 15297, 10112, 8969, 19028, 3276, 32251, 268, 12666, 8508, 268, 12931, 20649, 382, 41469, 305, 1885, 68, 11, 6754, 52427, 728, 5534, 31732, 0]\n",
    "- Tokenizer Meta-Llama-3-8B-Instruct-tokenizer (2.3MB) √ºber Moodle downloaden\n",
    "- unzip\n",
    "- Beispiel-Implementierung:\n",
    "    ```\n",
    "    token_ids = [...]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"/Users/michaelpfeiffer/notebooks_session2/Meta-Llama-3-8B-Instruct-tokenizer_folder/Unzipped/5f0b02c75b57c5855da9ae460ce51323ea669d8a\")\n",
    "    decoded_text = tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "    ```\n",
    "- **Gated Models auf Hugging Face, wie z.B. Llama**\n",
    "    - Was sind Gated Models? Nur √ºber Anmeldung auf Huggingface und Zustimmung der Nutzungsbedingungen beziehbar\n",
    "    - Warum? Herausgeber, z.B. Meta f√ºr Llama3, m√∂chten kontrollieren, wer Zugriff hat\n",
    "    - Oft gilt Lizenz nur f√ºr Forschung, Nicht-kommerzielle-Nutzung\n",
    "    - Damit akzeptiert man rechtliche Rahmenbedingungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gated Huggingface Model](assets/image07_gated-huggingface-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîì **[L√ñSUNG_2.2.02]** Welchen Text ergeben diese Token-IDs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Super! Die Token-IDs wurden in Text r√ºckkodiert! Aber was bedeutet das 'eot' in '<|eot_id|>'? End of Turn in einer Konversation?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Ja, ich bin froh, dass du die Token-IDs wiederentdeckt hast!\n",
      "\n",
      "Das \"eot\" im Text steht f√ºr \"End of Turn\", also der Abschluss einer Konversationsrunde. Es ist ein spezielles Token, das verwendet wird, um anzudeuten, dass eine Konversation beendet ist und keine weiteren Antworten oder Fragen erwartet werden.\n",
      "\n",
      "In verschiedenen Chat- und Sprachverkehrssystemen wird dieses Token verwendet, um den Fortschritt der Konversation zu kennzeichnen. Es ist wie ein \"Ende\" des Gespr√§chs, bei dem beide Parteien wissen, dass sie nicht mehr antworten oder fragen werden k√∂nnen.\n",
      "\n",
      "Ich hoffe, das hilft dir weiter!\n"
     ]
    }
   ],
   "source": [
    "token_ids = [128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 271, 128009, 128006, 882, 128007, 271, 19841, 0, 8574, 9857, 12, 31566, 48750, 304, 2991, 436, 19919, 74, 347, 17465, 0, 37674, 574, 82931, 295, 6754, 364, 68, 354, 6, 304, 364, 128009, 71090, 4060, 315, 12268, 304, 17495, 24277, 23124, 30, 128009, 128006, 78191, 128007, 271, 53545, 11, 10864, 9736, 13091, 71, 11, 15297, 3930, 2815, 9857, 12, 31566, 27348, 306, 34525, 83, 34143, 2268, 33717, 330, 68, 354, 1, 737, 2991, 55483, 7328, 330, 3812, 315, 12268, 498, 1101, 2761, 22855, 94483, 17495, 24277, 74095, 81, 29965, 13, 9419, 6127, 4466, 2395, 8510, 37907, 9857, 11, 6754, 75291, 15165, 11, 4543, 78968, 799, 13462, 11, 15297, 10021, 24277, 23124, 387, 55181, 6127, 2073, 32457, 31732, 268, 96917, 12666, 71877, 2781, 36708, 295, 12931, 382, 644, 89787, 13149, 12, 2073, 15883, 613, 424, 82212, 784, 615, 268, 15165, 47768, 9857, 75291, 11, 4543, 3453, 11246, 21740, 56013, 2761, 24277, 23124, 6529, 75361, 51691, 12778, 13, 9419, 6127, 13672, 4466, 330, 3812, 68, 1, 951, 21031, 652, 32673, 82, 11, 13582, 2486, 98244, 3744, 68, 3675, 76230, 11, 15297, 10112, 8969, 19028, 3276, 32251, 268, 12666, 8508, 268, 12931, 20649, 382, 41469, 305, 1885, 68, 11, 6754, 52427, 728, 5534, 31732, 0]\n",
    " \n",
    "from transformers import AutoTokenizer\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"5f0b02c75b57c5855da9ae460ce51323ea669d8a\")\n",
    "decoded_text = tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Versteckte Token-IDs: machen Struktur der Konversation klar, nicht sichtbar f√ºr Endnutzer:in, aber sehr wichtig f√ºr Modell\n",
    "    - <|start_header_id|>\n",
    "    - <|end_header_id|>\n",
    "    - <|eot_id|> -> \"end of turn\"-ID, das Ende einer Eingabe, sei es vom System, vom User oder vom Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text_2 = tokenizer.decode([0, 1, 2, 3, 4, 5, 6, 7, 20000, 0, 30929], skip_special_tokens=False)\n",
    "print(decoded_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text_3 = tokenizer.decode([3404,12509,527,12738,0], skip_special_tokens=True)\n",
    "print(decoded_text_3)\n",
    "\n",
    "decoded_text_3 = tokenizer.decode([3404], skip_special_tokens=True)\n",
    "print(decoded_text_3)\n",
    "decoded_text_3 = tokenizer.decode([12509], skip_special_tokens=True)\n",
    "print(decoded_text_3)\n",
    "decoded_text_3 = tokenizer.decode([527], skip_special_tokens=True)\n",
    "print(decoded_text_3)\n",
    "decoded_text_3 = tokenizer.decode([12738], skip_special_tokens=True)\n",
    "print(decoded_text_3)\n",
    "decoded_text_3 = tokenizer.decode([0], skip_special_tokens=True)\n",
    "print(decoded_text_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Konzept: Kontext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Die Liste \"context\" enth√§lt die exakte Abfolge der Token, so wie sie vom Modell verarbeitet wurden.\n",
    "- Bei einem Gespr√§ch mit Verlauf enth√§lt \"context\":\n",
    "    - Den ersten Systemprompt\n",
    "    - Fr√ºhere Nutzereingaben\n",
    "    - Fr√ºhere Modellantworten\n",
    "    - Die aktuelle Nutzereingabe\n",
    "    - Die aktuelle Modellantwort\n",
    "\n",
    "- Die ersten Token in \"context\" stammen vom Systemprompt.\n",
    "- Neue Prompts und Antworten werden der Liste in Reihenfolge angeh√§ngt.\n",
    "- Wenn es einen Gespr√§chsverlauf gibt, ist dieser vollst√§ndig in \"context\" enthalten ‚Äì vor dem neuesten Prompt.\n",
    "\n",
    "- Beispiel:\n",
    "    - Nutzer: ‚ÄûHallo, wie geht‚Äôs dir?‚Äú ‚Üí Tokenisierung: [1256, 890, 45, 289, 902]\n",
    "    - ‚Üí context: [1256, 890, 45, 289, 902]\n",
    "    - Modell: ‚ÄûMir geht‚Äôs gut!‚Äú ‚Üí Tokenisierung: [678, 45, 201, 90]\n",
    "    - ‚Üí context: [1256, 890, 45, 289, 902, 678, 45, 201, 90]\n",
    "\n",
    "- Aber:\n",
    "    - Es geht nicht nur um die reine Tokenisierung von Nutzereingabe und Modellantwort!\n",
    "    - Es sind auch System- bzw. Meta-Token enthalten, wie z.‚ÄØB.:\n",
    "        - < START >\n",
    "        - < STOP >\n",
    "        - < USER >\n",
    "        - < ASSISTANT >\n",
    "        - < CONTEXT >\n",
    "    - Diese sorgen daf√ºr, dass das Modell versteht, wer spricht, wo eine Antwort endet, und wie die Struktur des Gespr√§chs aussieht.\n",
    "\n",
    "- Die Token-IDs unseres Kontextes, der √ºber API mitgeschickt wurde, k√∂nnen wir mit einem Tokenizer wieder in Text r√ºckverwandeln!\n",
    "- Daf√ºr m√ºssen wir uns den Tokenizer holen - f√ºr Llama3.2 z.B. √ºber HuggingFace - hier einloggen und Meta-Nutzerbestimmungen f√ºr Llama zustimmen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Konzept: Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- durch Mitgabe eines Parameters wie dem Key 'format' oder 'response_format' und dem Value 'json' und einem Schema dazu wird Structured Output erm√∂glicht\n",
    "- Wie funktioniert das und warum ist zu 100% sicher, dass die API-Response Syntax-korrekt in JSON ist?\n",
    "    - 1. Request mit Parameter 'format'/'response_format'/... und Schema schicken\n",
    "    - 2. Schema wird nicht als Teil des Prompts sichtbar, sondern im Hintergrund an Modell √ºbergeben -> interner System-Prompt wird erzeugt \"Yom must respond ONLY with a JSON object ...\"\n",
    "        - Hier kann sich das Modell noch immer daneben benehmen\n",
    "    - 3. Ein Zwischensystem √ºberpr√ºft die Antwort - falls die nicht korrekt ist, wird ein Retry gemacht bzw. kann in manchen F√§llen automatisch repariert/geparst werden\n",
    "    - 4. Als Nutzer:in bekommt man ein 100% wohlgeformtes JSON\n",
    "- ohne Structured Output und nur Anweisung in Prompt? Unsicher, oft mit z.B. \"Hier ist dein JSON: ...\" -> kann programmatisch nicht zu 100% sicher weitergegeben werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Ollama Endpunkt (A) 'Generate a completion' POST /api/generate: Mit Structured Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a completion: structured output\n",
    "\n",
    "# --------------\n",
    "# vars\n",
    "# --------------\n",
    "endpoint = config.OAPI_GENERATE_URL\n",
    "model = config.OMODEL_LLAMA3D2\n",
    "prompt = \"Mein Name ist Rita Raster und ich bin Programmiererin. Ich bin 1995 geboren und liebe Musik machen. Ich habe nie Zeit. Antworte in JSON\"\n",
    "stream_bool = False\n",
    "format_input = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"Vorname\": {\"type\": \"string\"},\n",
    "        \"Nachname\": {\"type\": \"string\"},\n",
    "        \"Beruf\": {\"type\": \"string\"},\n",
    "        \"Geburtsjahr\": {\"type\": \"integer\"},\n",
    "        \"Hobbies\": {\"type\": \"array\"},\n",
    "        \"Verf√ºgbarkeit\": {\"type\": \"boolean\"}\n",
    "    },\n",
    "    \"required\": [\n",
    "        \"Vorname\",\n",
    "        \"Beruf\",\n",
    "        \"Geburtsjahr\",\n",
    "        \"Hobbies\",\n",
    "        \"Verf√ºgbarkeit\"\n",
    "    ]\n",
    "  }\n",
    "\n",
    "# --------------\n",
    "# execution\n",
    "# --------------\n",
    "# Request Data:\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": stream_bool,\n",
    "    \"format\": format_input\n",
    "}\n",
    "# Send Request and Jsonify it, to be able to print it:\n",
    "response = requests.post(endpoint, json=data)\n",
    "response_json = response.json()\n",
    "\n",
    "# --------------\n",
    "# output\n",
    "# --------------\n",
    "# Print API-response:\n",
    "#print(json.dumps(response_json, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Print only model-reponse:\n",
    "# a) Extract the response as a string\n",
    "response_text = response_json[\"response\"]\n",
    "# b) Convert the string back into a Python dictionary\n",
    "response_data = json.loads(response_text)\n",
    "# c) Print the formatted JSON response\n",
    "print(json.dumps(response_data, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[√úBUNG_2.2.03]** Structured Output generieren, mit Pydantic statt pure JSON Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Diese √úbung ist Teil von √úBUNG3. Speichere deine Ergebnisse und Notizen in einem File (Word), f√ºge dann noch die restlichen √úbungen dieser Session 2.2 hinzu und lade sie auf Moodle unter \"Abgabe √úbung 3\" bis zum 20.05.25 hoch.*\n",
    "- Probiere verschiedene Prompts mit verschiedenen JSON-Schemas f√ºr Structured Output und bekomme ein Gef√ºhl f√ºr St√§rken/Schw√§chen\n",
    "- Probiere es mit einer Pydantic Klasse als input -> der Format Parameter kann keine Pydantic-Klasse annehmen, aber es g√§be einen Weg\n",
    "- Warum der Name \"Pydantic\"? \"Py\" von Python, \"dentic\" von \"pedantic\" - √ºbertrieben detailverliebt und regelkonform auf Korrektheit und Pr√§zsion achten -> Pydantic dient zur Datenvalidierung und Typisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[L√ñSUNG_2.2.03]** Structured Output generieren, mit Pydantic statt pure JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a completion: structured output\n",
    "\n",
    "# --------------\n",
    "# vars\n",
    "# --------------\n",
    "endpoint = config.OAPI_GENERATE_URL\n",
    "model = config.OMODEL_LLAMA3D2\n",
    "prompt = \"Mein Name ist Rita Raster und ich bin Programmiererin. Ich bin 1995 geboren und liebe Musik machen. Ich habe nie Zeit. Antworte in JSON\"\n",
    "stream_bool = False\n",
    "class PersonInput(BaseModel):\n",
    "    Vorname: str\n",
    "    Nachname: Optional[str] = None\n",
    "    Beruf: str\n",
    "    Geburtsjahr: int\n",
    "    Hobbies: List[str]\n",
    "    Verf√ºgbarkeit: bool\n",
    "\n",
    "# --------------\n",
    "# execution\n",
    "# --------------\n",
    "# Request Data:\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": stream_bool,\n",
    "    \"format\": PersonInput.model_json_schema() # hier die Umwandlung auf JSON-Schema; mit OpenAI-SDK kann man direkt Pydantic-Klasse belassen\n",
    "}\n",
    "# Send Request and Jsonify it, to be able to print it:\n",
    "response = requests.post(endpoint, json=data)\n",
    "response_json = response.json()\n",
    "\n",
    "# --------------\n",
    "# output\n",
    "# --------------\n",
    "# Print API-response:\n",
    "#print(json.dumps(response_json, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Print only model-reponse:\n",
    "# a) Extract the response as a string\n",
    "response_text = response_json[\"response\"]\n",
    "# b) Convert the string back into a Python dictionary\n",
    "response_data = json.loads(response_text)\n",
    "# c) Print the formatted JSON response\n",
    "print(json.dumps(response_data, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Konzept: Multimodale Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Beispiel Text-Prompt mit Bild\n",
    "    - prompt = \"What can you see in the picture?\"\n",
    "    - images = [sunset.jpg]\n",
    "    - ```[<vision_start>, [EMBEDDINGS_DES_BILDS], <vision_end>, <text_start>, \"What can you see in the picture?\", <text_end>]```\n",
    "    - Ein visuelles Embedding wird in das Kontextfenster eingespeist, durch Vision-Encoder (z.B. CLIP-ViT, ResNet, Swin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Ollama Endpunkt (A) 'Generate a completion' POST /api/generate: Mit Image als weiteren Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# vars\n",
    "# --------------\n",
    "endpoint = config.OAPI_GENERATE_URL\n",
    "model = config.OMODEL_LLAVA\n",
    "prompt = \"What can you see in the picture?\"\n",
    "stream_bool = False\n",
    "image_path = config.ASSETS_PATH + \"sunset.jpg\"\n",
    "image_base64 = encode_image(image_path)\n",
    "\n",
    "# --------------\n",
    "# funcs\n",
    "# --------------\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# --------------\n",
    "# execution\n",
    "# --------------\n",
    "# Request Data:\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": stream_bool,\n",
    "    \"images\": [image_base64]\n",
    "}\n",
    "# Send Request and Jsonify it, to be able to print it:\n",
    "response = requests.post(endpoint, json=data)\n",
    "response_json = response.json()\n",
    "\n",
    "# --------------\n",
    "# output\n",
    "# --------------\n",
    "# Print API-response:\n",
    "#print(json.dumps(response_json, indent=4, ensure_ascii=False))\n",
    "# Print only model-reponse:\n",
    "response_text = response_json[\"response\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Ollama Endpunkt (A) 'Generate a completion' POST /api/generate: Mit Temperatur-, Top-K- und Top-P-Unterschiede "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- temperature, top_p, top_k resource and tool for checking: https://artefact2.github.io/llm-sampling/index.xhtml\n",
    "- temperature: 0-1 und dar√ºber\n",
    "- top_p: 0-1 (wo ist die Grenze des probibilistischen Durchschnitts pro Wort)\n",
    "- top_k: 0-1000 (wieviele der W√∂rter sollen inkludiert sein)\n",
    "    - Temperature: Ein Wert von 0,7 wird h√§ufig verwendet, um ein ausgewogenes Verh√§ltnis zwischen Kreativit√§t und Koh√§renz zu erreichen.\n",
    "    - Top_k: Ein Wert von 50 wird oft gew√§hlt, um die Auswahl auf die 50 wahrscheinlichsten Token zu beschr√§nken und so die Vielfalt zu erh√∂hen. Ôøº\n",
    "    - Top_p: Ein Wert von 0,9 bedeutet, dass die Auswahl der n√§chsten Token so erfolgt, dass die kumulative Wahrscheinlichkeit 90 % erreicht, was zu vielf√§ltigeren und weniger repetitiven Antworten f√ºhrt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a completion: options - temperature\n",
    "\n",
    "# --------------\n",
    "# vars\n",
    "# --------------\n",
    "endpoint = config.OAPI_GENERATE_URL\n",
    "model = config.OMODEL_LLAMA3D2\n",
    "prompt = \"Give me 5 random words.\"\n",
    "#prompt = \"How is apple cake made? Describe it in 6 sentences.\"\n",
    "stream_bool = False\n",
    "high_randomness = True\n",
    "\n",
    "# --------------\n",
    "# execution\n",
    "# --------------\n",
    "# randomness:\n",
    "temperature_low_randomness = 0.01\n",
    "top_p_low_randomness = 0.05\n",
    "top_k_low_randomness = 1\n",
    "temperature_high_randomness = 1.9\n",
    "top_p_high_randomness = 0.99\n",
    "top_k_high_randomness = 1000\n",
    "\n",
    "if high_randomness == True:\n",
    "    temperature = temperature_high_randomness\n",
    "    top_p = top_p_high_randomness\n",
    "    top_k = top_k_high_randomness\n",
    "else:\n",
    "    temperature = temperature_low_randomness\n",
    "    top_p = top_p_low_randomness\n",
    "    top_k = top_k_low_randomness\n",
    "\n",
    "# Request Data:\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"stream\": stream_bool,\n",
    "    \"options\": {\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"seed\": None\n",
    "    },\n",
    "}\n",
    "# Send Request and Jsonify it, to be able to print it:\n",
    "response = requests.post(endpoint, json=data)\n",
    "response_json = response.json()\n",
    "\n",
    "# --------------\n",
    "# output\n",
    "# --------------\n",
    "# Print API-response:\n",
    "#print(json.dumps(response_json, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Print only model-reponse:\n",
    "print(response_json[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Ollama Endpunkt (A) 'Generate a completion' POST /api/generate: System-Prompt anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a completion: system\n",
    "\n",
    "# --------------\n",
    "# vars\n",
    "# --------------\n",
    "endpoint = config.OAPI_GENERATE_URL\n",
    "model = config.OMODEL_LLAMA3D2\n",
    "prompt = \"What is a train?\"\n",
    "#prompt = '''Here is a question and an answer:\n",
    "#    Question: How old are you?\n",
    "#    Answer: I live in California.\n",
    "#    Does the answer seem valid? Reply with 'Yes' or 'No' and explain briefly.'''\n",
    "system = \"You are an old man and communicate in that way.\"\n",
    "stream_bool = False\n",
    "\n",
    "# --------------\n",
    "# execution\n",
    "# --------------\n",
    "# Request Data:\n",
    "data = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": prompt,\n",
    "    \"system\": system,\n",
    "    \"stream\": stream_bool\n",
    "}\n",
    "# Send Request and Jsonify it, to be able to print it:\n",
    "response = requests.post(endpoint, json=data)\n",
    "response_json = response.json()\n",
    "\n",
    "# --------------\n",
    "# output\n",
    "# --------------\n",
    "print(response_json[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[√úBUNG_2.2.04]** System-Prompt-Beeinflussung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Diese √úbung ist Teil von √úBUNG3. Speichere deine Ergebnisse und Notizen in einem File (Word), f√ºge dann noch die restlichen √úbungen dieser Session 2.2 hinzu und lade sie auf Moodle unter \"Abgabe √úbung 3\" bis zum 20.05.25 hoch.*\n",
    "- Versuche verschiedene System-Prompts mit anschlie√üenden Prompts und beobachte die LLM-Antworten\n",
    "- Wenn du an realistische Use-Cases denkst - was k√∂nnte Sinn machen, in den System-Prompt zu schreiben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Ollama Endpunkt (B) 'Generate a chat completion' POST /api/chat: Einfache Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter**\n",
    "- `model`: (required) the model name\n",
    "- `messages`: the messages of the chat, this can be used to keep a chat memory\n",
    "- `tools`: list of tools in JSON for the model to use if supported\n",
    "\n",
    "The message object has the following fields:\n",
    "- `role`: the role of the message, either system, user, assistant, or tool\n",
    "- `content`: the content of the message\n",
    "- `images` (optional): a list of images to include in the message (for multimodal models such as llava)\n",
    "- `tool_calls` (optional): a list of tools in JSON that the model wants to use\n",
    "\n",
    "**Weitere Parameter**\n",
    "- `format`: the format to return a response in. Format can be json or a JSON schema.\n",
    "- `options`: additional model parameters listed in the documentation for the Modelfile such as temperature\n",
    "- `stream`: if false the response will be returned as a single response object, rather than a stream of objects\n",
    "- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: 5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------\n",
    "# vars\n",
    "# --------------\n",
    "endpoint = config.OAPI_CHAT_URL\n",
    "model = config.OMODEL_LLAMA3D2\n",
    "messages = ''\n",
    "prompt1 = \"Tell me a small joke.\"\n",
    "prompt2 = \"Explain shortly why it is funny.\"\n",
    "stream_bool = False\n",
    "\n",
    "# --------------\n",
    "# funcs\n",
    "# --------------\n",
    "# Initialize chat history\n",
    "chat_memory = []\n",
    "\n",
    "def send_chat_message(user_input, stream=False):\n",
    "    global chat_memory  # Keep memory of the conversation\n",
    "\n",
    "    # Append user message to chat history\n",
    "    chat_memory.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Define the payload for the API request\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": chat_memory,\n",
    "        \"stream\": stream  # Control streaming (True or False)\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(endpoint, json=payload)\n",
    "\n",
    "    # Parse the response\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        assistant_message = response_json.get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "        # Append assistant response to memory\n",
    "        chat_memory.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
    "\n",
    "        return assistant_message\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "# --------------\n",
    "# execution\n",
    "# --------------\n",
    "# Send a test message\n",
    "response_1 = send_chat_message(prompt1)\n",
    "#print(\"Assistant:\", response_1)\n",
    "\n",
    "# Continue conversation\n",
    "response_2 = send_chat_message(prompt2)\n",
    "#print(\"Assistant:\", response_2)\n",
    "\n",
    "# --------------\n",
    "# output\n",
    "# --------------\n",
    "# Print conversation history\n",
    "print(\"\\nChat History:\")\n",
    "for msg in chat_memory:\n",
    "    print(f\"{msg['role'].capitalize()}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Konzept: Konversations-Historie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nett, wenn der Kontext eine System-Message und eine User-Message hat und sich an alles vorhergehende erinnern kann\n",
    "- Beispiel:\n",
    "    ```\n",
    "    conversation_history = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that loves programming jokes.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Hi there! How can I assist you today?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a programming joke.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Why do programmers prefer dark mode? Because light attracts bugs! üêõ\"},\n",
    "        {\"role\": \"user\", \"content\": \"Haha, that's a good one. Can you give me another?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Sure! Why do Java developers wear glasses? Because they don‚Äôt C#! üòÑ\"}\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[√úBUNG_2.2.05]** Mini-Chat-Applikation mit Conversation History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Diese √úbung ist Teil von √úBUNG3. Speichere deine Ergebnisse und Notizen in einem File (Word), f√ºge dann noch die restlichen √úbungen dieser Session 2.2 hinzu und lade sie auf Moodle unter \"Abgabe √úbung 3\" bis zum 20.05.25 hoch.*\n",
    "\n",
    "TODO:\n",
    "- Es soll m√∂glich sein, hier im Notebook oder wo du auch immer willst nach Klick auf \"Play\" der Zelle mit dem lokalen LLM zu chatten\n",
    "- Sich abwechselnde Ausgaben \"You\", \"Assistant\", \"Token Usage\", wobei die/der User:in bei \"You\" den Content (Prompt) eingeben kann\n",
    "- LLM soll alles von System-Prompt bis hin zu aktuellen Completions alles in der conversation_history mitnehmen\n",
    "\n",
    "Fragen:\n",
    "- Welche Kontextfenster-Gr√∂√üe hat das benutzte Modell - i.e. wieviele Tokens passen maximal in die History?\n",
    "- Was passiert, wenn das Limit des Kontextfensters erreicht wurde?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[L√ñSUNG_2.2.05]** Notebook-Chat-Applikation mit Conversation History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input zu TODOs:\n",
    "- normalerweise funktioniert user_input = input(\"Please type something\") -> direkt in Jupyter Notebook\n",
    "- in VSC besser mit ipywidgets arbeiten\n",
    "\n",
    "Antworten auf Fragen:\n",
    "- Welche Kontextfenster-Gr√∂√üe hat das benutzte Modell - i.e. wieviele Tokens passen maximal in die History?\n",
    "    - mit z.B. 'ollama show llama3.2:latest' sieht man den Parameter 'context length', in diesem Fall 131072.\n",
    "- Was passiert, wenn das Limit des Kontextfensters erreicht wurde?\n",
    "    - Beim √úberschreiten der Kontextgrenze wird die History von vorne gek√ºrzt, Ollama macht das z.B. automatisch\n",
    "    - Es gibt smarte K√ºrzungstechniken, die man anwenden kann (z.B. zusammenfassen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert custom CSS for styling messages\n",
    "display(HTML('''\n",
    "<style>\n",
    "    .user-message {\n",
    "        background-color: #e6f7ff;\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "        margin-bottom: 5px;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f0f0f0;\n",
    "        padding: 10px;\n",
    "        border-radius: 5px;\n",
    "        margin-bottom: 5px;\n",
    "    }\n",
    "    .token-usage {\n",
    "        background-color: #fff7e6;\n",
    "        padding: 5px;\n",
    "        border-radius: 5px;\n",
    "        margin-bottom: 5px;\n",
    "        font-size: 0.9em;\n",
    "        color: #333;\n",
    "    }\n",
    "</style>\n",
    "'''))\n",
    "\n",
    "# Set up logging to file only\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    handlers=[logging.FileHandler(\"llm_chat_log.txt\")]\n",
    ")\n",
    "\n",
    "conversation_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "# Function to call LLM and return reply + usage\n",
    "def chat_with_llm(prompt):\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    model = \"llama3.2:latest\"\n",
    "    response = openaisdk_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=conversation_history\n",
    "    )\n",
    "    \n",
    "    usage = response.usage\n",
    "    if usage:\n",
    "        logging.info(\n",
    "            f\"Token usage ‚Äî Prompt: {usage.prompt_tokens}, \"\n",
    "            f\"Completion: {usage.completion_tokens}, \"\n",
    "            f\"Total: {usage.total_tokens}\"\n",
    "        )\n",
    "        usage_text = (\n",
    "            f\"üí° Token usage ‚Äî \"\n",
    "            f\"Prompt: `<code>{usage.prompt_tokens}</code>`, \"\n",
    "            f\"Completion: `<code>{usage.completion_tokens}</code>`, \"\n",
    "            f\"Total: `<code>{usage.total_tokens}</code>`\"\n",
    "        )\n",
    "    else:\n",
    "        usage_text = \"‚ö†Ô∏è No token usage info available.\"\n",
    "\n",
    "    assistant_reply = response.choices[0].message.content\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
    "    \n",
    "    return assistant_reply, usage_text\n",
    "\n",
    "# Widgets\n",
    "input_box = widgets.Text(\n",
    "    placeholder='Type your message',\n",
    "    description='You:',\n",
    "    disabled=False\n",
    ")\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "clear_button = widgets.Button(description=\"Clear\")\n",
    "status_label = widgets.Label(value=\"\")\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Layout\n",
    "input_controls = widgets.HBox([input_box, send_button, clear_button])\n",
    "ui = widgets.VBox([output_area, status_label, input_controls])\n",
    "display(ui)\n",
    "\n",
    "# Button callbacks\n",
    "def on_send_clicked(b):\n",
    "    user_input = input_box.value.strip()\n",
    "    if not user_input:\n",
    "        return\n",
    "    input_box.value = ''\n",
    "    \n",
    "    with output_area:\n",
    "        display(HTML(f'<div class=\"user-message\"><strong>You:</strong> {user_input}</div>'))\n",
    "    \n",
    "    if user_input.lower() == 'exit':\n",
    "        with output_area:\n",
    "            display(HTML('<div class=\"assistant-message\"><strong>Goodbye!</strong></div>'))\n",
    "        send_button.disabled = True\n",
    "        input_box.disabled = True\n",
    "        return\n",
    "    \n",
    "    status_label.value = \"Generating answer...\"\n",
    "    assistant_reply, usage_text = chat_with_llm(user_input)\n",
    "    status_label.value = \"\"\n",
    "    \n",
    "    with output_area:\n",
    "        display(HTML(f'<div class=\"assistant-message\"><strong>Assistant:</strong> {assistant_reply}</div>'))\n",
    "        display(HTML(f'<div class=\"token-usage\">{usage_text}</div>'))\n",
    "\n",
    "def on_clear_clicked(b):\n",
    "    output_area.clear_output()\n",
    "    send_button.disabled = False\n",
    "    input_box.disabled = False\n",
    "\n",
    "send_button.on_click(on_send_clicked)\n",
    "clear_button.on_click(on_clear_clicked)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
