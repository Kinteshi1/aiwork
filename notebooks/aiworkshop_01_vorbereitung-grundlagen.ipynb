{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.1: LLMs, Ollama, virtuelle Umgebung und IDE mit Kernel aufsetzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wie funktionieren LLMs?\n",
    "    - Youtube \"Deep Dive into LLMs like ChatGPT (Andrej Karpathy)\": https://www.youtube.com/watch?v=7xTGNNLPyMI&t=11715s\n",
    "    - Youtube \"Create a Large Language Model from Scratch with Python ‚Äì Tutorial (freeCodeCamp)\": https://www.youtube.com/watch?v=UU1WVnMk4E8\n",
    "        - Github-Resource dazu: https://github.com/Infatoshi/fcc-intro-to-llms\n",
    "    - Github LLMs-from-scratch https://github.com/rasbt/LLMs-from-scratch\n",
    "\n",
    "- Ollama: https://ollama.com/ (Github https://github.com/ollama/ollama)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama als lokales Model Serving aufsetzen und Models pullen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Windows -> besser WSL als Powershell!\n",
    "- Ollama installieren: https://github.com/ollama/ollama\n",
    "    - Windows: OllamaSetup.exe\n",
    "    - MacOS: Ollama-darwin.zip oder z.B. brew install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ollama Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ollama Models](assets/image09_ollama-models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Starten von ollama mit Befehl 'ollama serve' - API nun lokal vorhanden (Default: Listening on 127.0.0.1:11434, Basis-Endpunkt: http://localhost:11434/v1)\n",
    "- In einem neuen Terminal-Tab erstes Modell pullen und damit experimentieren: 'ollama run llama3.2:3b', Gr√∂√üe 2GB (https://ollama.com/library/llama3.2:3b)\n",
    "- Alternativ nur Modell pullen mit 'ollama pull llama3.2:3b'\n",
    "- Modell wieder l√∂schen mit 'ollama rm llama3.2:3b'\n",
    "- Mit 'ollama list' sehen, welche Models lokal zur Verf√ºgung stehen\n",
    "- Mit 'ollama show' Modell-Spezifika sehen\n",
    "- Die besten Generierungsmodelle f√ºr Deutsch empfohlen von einer:m Reddit-User:in: https://www.reddit.com/r/LocalLLaMA/comments/1gayvch/what_are_the_best_models_for_use_with_german/ -> Mistral Nemo 12b\n",
    "    - *'mistral-nemo 12b'* auf Ollama: https://ollama.com/library/mistral-nemo\n",
    "- Das beste Embedding-Modell f√ºr Deutsch laut Leaderboard (https://huggingface.co/spaces/mteb/leaderboard) -> *'multilingual-e5-large-instruct'*\n",
    "    - *'multilingual-e5-large-instruct'* auf Ollama: https://ollama.com/jeffh/intfloat-multilingual-e5-large-instruct\n",
    "\n",
    "```\n",
    "USER% ollama list\n",
    "NAME                                                 ID              SIZE      MODIFIED\n",
    "jeffh/intfloat-multilingual-e5-large-instruct:f32    1ae6fc952be9    2.2 GB    ... ago   # embedding\n",
    "mistral-nemo:latest                                  994f3b8b7801    7.1 GB    ... ago   # generation\n",
    "llava:latest                                         8dd30f6b0cb1    4.7 GB    ... ago   # generation (vision)\n",
    "llama3.2:latest                                      a80c4f17acd5    2.0 GB    ... ago   # generation\n",
    "deepseek-r1:14b                                      ea35dfe18182    9.0 GB    ... ago   # generation with reasoning\n",
    "```\n",
    "\n",
    "```\n",
    "USER% df -h ~/.ollama\n",
    "Filesystem      Size    Used   Avail Capacity iused ifree %iused  Mounted on\n",
    "/dev/disk3s1   460Gi    80Gi   350Gi    19%    598k  3.7G    0%   /System/Volumes/Data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Virtuelle Umgebung aufsetzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'uv' Python Package und Project Manager (in Rust geschrieben) installieren: https://github.com/astral-sh/uv\n",
    "- sicher gehen, dass PATH-Variable den Pfad der uv-Installation ber√ºcksichtigt\n",
    "  - ```michaelpfeiffer@Michaels-MacBook-Pro ~ % which uv```\n",
    "      ```/Users/michaelpfeiffer/.local/bin/uv```\n",
    "  - ```export PATH=\"$HOME/.local/bin:$PATH\"```\n",
    "  - alternativ diese Zeile noch in bshrc oder zshrc hinzuf√ºgen : ```nano ~/.zshrc```\n",
    "- Ordner \"aiworkshop\" im Repo- bzw. Projekte-Folder anlegen, in diesen Ordner gehen und 'uv init' ausf√ºhren\n",
    "- Im File .python-version die Version auf 3.11 setzen bzw. einfach Command 'uv python pin 3.11'\n",
    "- Im File pyproject.toml requires-python auf \">=3.11\" setzen und diese Dependencies hinzuf√ºgen:\n",
    "\n",
    "```\n",
    "dependencies = [\n",
    "  \"asyncio\",\n",
    "  \"numpy\",\n",
    "  \"requests\",\n",
    "  \"pymilvus\",\n",
    "  \"openai\",\n",
    "  \"openai-agents\",\n",
    "  \"duckduckgo-search\",\n",
    "  \"nest-asyncio\",\n",
    "  \"pydantic\",\n",
    "  \"httpx\",\n",
    "  \"jupyterlab\",\n",
    "  \"python-dotenv\",\n",
    "  \"pip\",\n",
    "  \"setuptools\",\n",
    "  \"wheel\"\n",
    "]\n",
    "```\n",
    "\n",
    "- im Terminal in diesem Ordner dann 'uv sync' ausf√ºhren: kreiert virtuelle Umgebung mit einem .venv Ordner\n",
    "- falls kein venv-Ordner kreiert wurde -> 'uv venv'\n",
    "- Wie das Venv aktivieren? 'source .venv/bin/activate' im Terminal, deaktivieren mit 'deactivate'\n",
    "- Sehen, welche Venvs durch 'uv' erstellt worden sind: 'find ~/Projects -name \".venv\" -type d'\n",
    "- Sehen, wie gro√ü eine Venv ist: 'du -sh /Users/michaelpfeiffer/Projects/personal/aiworkshop/.venv' -> z.B. 1.3G\n",
    "- Venv entfernen durch 'uv remove'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDE mit Kernel f√ºr Notebooks aufsetzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VSC oder Jupyter oder gew√ºnschte IDE\n",
    "- VSC-Extensions installieren: Jupyter Extension, Markdown Preview Mermaid Support\n",
    "- wenn gew√ºnscht, Git konfigurieren und lokal commiten\n",
    "- Kernel registrieren im VSC-Terminal mit 'python -m ipykernel install --user --name=aiworkshop --display-name \"Python (aiworkshop)\"'\n",
    "- VSC neu starten\n",
    "- venv des uv-Managers aktivieren im VSC-Terminal ('source .venv/bin/activate', wenn man sich im Projektordner befindet)\n",
    "- Kernel in Notebook ausw√§hlen\n",
    "- wenn eine Dependency dazukommt, einfach mit 'uv sync' aktualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aiworkshop_utils Module erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __init__.py\n",
    "- config.py\n",
    "- custom_utils.py\n",
    "- docling_imports.py\n",
    "- embedders.py\n",
    "- jupyter_imports.py\n",
    "- openai_imports.py\n",
    "- standardlib_imports.py\n",
    "- thirdparty_imports.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[Teil-√úBUNG 2.1.01]** Im Terminal mit Ollama Model chatten (nicht Abgabe-relevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fragen:\n",
    "\n",
    "- Generell\n",
    "    - Welche verschiedenen Model-Arten gibt es? Was ist z.B. ein Reasoning-Modell mit der CoT (Chain-of-Thought) Methode?\n",
    "    - In welchen Dateiformaten werden diese Modelle abgespeichert (z.B. GGUF) und woraus bestehen sie (z.B. Weights und Biases, was noch?)?\n",
    "\n",
    "- Ollama-bezogen\n",
    "    - Welche Models werden auf Ollama angeboten? Bringt man jedes Modell von Huggingface z.B. auf Ollama?\n",
    "    - Kann man √ºber Ollama Models auch selbst finetunen?\n",
    "    - Welche Optionen gibt es nach 'run' (mit '/?' anzeigen)?\n",
    "    - Werden alle Messages im Kontext gespeichert?\n",
    "    - Was ist \"Wordwrap\", \"Format json\" und \"Verbose\"?\n",
    "    - Was kann man mit dem Ollama Modelfile machen, welchen Zweck hat es?\n",
    "    - Wohin werden die Ollama Models geladen, welche Gr√∂√üen haben die Models ca.?\n",
    "\n",
    "- Performance\n",
    "    - Wieviel GPU / CPU wird bei Ben√ºtzung genutzt? Logging mit Test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Ollama Terminal](assets/image10_ollama-terminal.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aiworkshop)",
   "language": "python",
   "name": "aiworkshop"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
