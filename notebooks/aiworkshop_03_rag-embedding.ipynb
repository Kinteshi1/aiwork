{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2.3: Embedding, Vektordatenbanken und RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Huggingface MTEB (Massive Text Embedding Benchmark) Leaderboard: https://huggingface.co/spaces/mteb/leaderboard\n",
    "    - Mit Auswahl Language 'deu' und Parameter 0-5000M -> *'multilingual-e5-large-instruct'* an erster Stelle\n",
    "    - Was ist MTEB?: https://huggingface.co/blog/mteb#benchmark-your-model\n",
    "    - *'multilingual-e5-large-instruct'* auf Ollama: https://ollama.com/jeffh/intfloat-multilingual-e5-large-instruct\n",
    "- Bestenliste auf Ollama -> Models -> Embeddings jedoch *'nomic-embed-text'*\n",
    "    - *'nomic-embed-text'* auf Ollama: https://ollama.com/library/nomic-embed-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiworkshop_utils.standardlib_imports import os, json, base64, logging, Optional, List, pprint, shutil, glob\n",
    "from aiworkshop_utils.thirdparty_imports import AutoTokenizer, load_dotenv, requests, BaseModel, Field, pd, cosine_similarity, plt, np, DataType, MilvusClient, tqdm\n",
    "from aiworkshop_utils.custom_utils import show_pretty_json, encode_image, load_md_sections_from_path\n",
    "from aiworkshop_utils.jupyter_imports import Markdown, HTML, widgets, display, JSON\n",
    "from aiworkshop_utils.openai_imports import OpenAI\n",
    "from aiworkshop_utils import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Konzept: Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was ist Embedding?\n",
    "    - Sprache (oder allgemein Daten) in Zahlen verwandeln\n",
    "    - Wie wir schon gelernt haben, wird Text in Token-IDs umgewandelt, z.B. \"Hi!\" ergibt wie vorher zwei Token-IDs: [13347, 0]\n",
    "    - Damit die Semantik eines Textes gedeutet werden kann, muss der Text eingebettet werden\n",
    "    - Beispiel f√ºr ein 8D-Embedding: \"Hi!\" -> Token-IDs [13347, 0] -> Embedding als z.B. 8D-Vektor [0.12, -0.58, 0.33, 0.77, -0.41, 0.05, 0.99, -0.22]\n",
    "    - Egal wie lange der Input ist - es wird immer ein Vektor mit gleicher Dimensionen-Anzahl erstellt\n",
    "    - Die Bedeutung des Textes, nicht seine L√§nge oder Struktur werden eingebettet - Mehr Kontext im Text, bessere Einbettung\n",
    "    - Vektoren g√§ngiger Modelle haben viele Dimensionen: GPT-3 12.288, GPT-4 bis zu 16.000 oder dar√ºber\n",
    "    - Jede Dimension ist eine gelernte semantische Eigenschaft, die wir nur erahnen k√∂nnen, z.B. \"Stimmung\", \"Humor\", \"Tier\", ...\n",
    "    - Je mehr Dimensionen, desto granularere semantische Feinunterscheidung\n",
    "    - Daten m√ºssen in den gleichen Vektorraum eingebettet werden, um die richtige semantische relative Distanz beizubehalten zwischen verschiedenen Daten\n",
    "    - Ein Prompt, der auf Daten abzielt, muss im selben Vektorraum eingebettet werden, um die semantisch √§hnlichsten Daten dazu richtig zu finden\n",
    "    - Es gibt spezialisierte Embedding-Modelle, die eher gut in Embedding sind und nicht in der Generierung\n",
    "\n",
    "- Was wird eigentlich eingebettet?\n",
    "    - Levels: Word / Sentence / Document\n",
    "    - Wort-Embeddings\n",
    "        - Beispiel: [\"K√∂nig\", \"K√∂nigin\", \"Mann\", \"Frau\"]\n",
    "        - Methoden: Word2Vec, GloVe, FastText\n",
    "        - Problem: Ein Wort gibt nichts √ºber Kontext Preis -> Bank -> Sitzbank? Institution?\n",
    "    - Satz-Embeddings\n",
    "        - Beispiel: [‚ÄûDie Bank ist bequem.‚Äú, ‚ÄûIch gehe zur Bank, um Geld abzuheben.‚Äú]\n",
    "        - Methoden: BERT, Sentence-BERT, Universal Sentence Encoder\n",
    "    - Document-Embeddings\n",
    "        - Beispiel: Zeitungsartikel\n",
    "        - Methode: Oft wird ein Dokumentembedding aus Satz-Embeddings aggregiert (z.‚ÄØB. Mittelwert, Attention, oder mit speziellen Modellen wie Longformer etc.)\n",
    "\n",
    "- Auf was ist zu achten?\n",
    "    - Verschiedene Modelle haben verschiedene Dimensionen\n",
    "        - Nomic hat z.B. 768 (12 Attention Heads x 64 Dimensionen pro Head)\n",
    "        - BERT large hat 1024 (16 Attention Heads x 64 Dimensionen pro Head)\n",
    "    - Die Vektordatenbank muss mit dieser Dimensions-Anzahl √ºbereinstimmen\n",
    "    - Multilingualit√§t - bei Nomic vorhanden\n",
    "    - Man kann √ºber Python-Library sentence-transformer einbetten oder √ºber Ollama Embedding gleich √ºber API bereitstellen und auslagern - machen wir!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama Embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ollama Embedding Request\n",
    "    - POST to \"http://localhost:11434/api/embed\"\n",
    "    - requ -> model: name of model to generate embeddings from\n",
    "    - requ -> input: text or list of text to generate embeddings for\n",
    "    - opti -> truncate: truncates the end of each input to fit within context length.\n",
    "        - Returns error if false and context length is exceeded. Defaults to true\n",
    "    - opti -> options: additional model parameters listed in the documentation for the Modelfile such as temperature\n",
    "    - opti -> keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m)\n",
    "\n",
    "- Ollama Embedding Response\n",
    "    - 'model': '...'\n",
    "    - 'embeddings': [[...,...,...,...]]\n",
    "    - 'total_duration': '...'\n",
    "    - 'load_duration': '...'\n",
    "    - 'prompt_eval_count': '...'\n",
    "\n",
    "- further sources:\n",
    "    - ollama general: https://github.com/ollama/ollama?tab=readme-ov-file\n",
    "    - ollama API: https://github.com/ollama/ollama/blob/main/docs/api.md#generate-embeddings\n",
    "    - ollama Modelfile: https://github.com/ollama/ollama/blob/main/docs/modelfile.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaEmbedder:\n",
    "    def __init__(self, url, model_name):\n",
    "        self.url = url\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def _post_request(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        response = requests.post(\n",
    "            self.url,\n",
    "            json={\n",
    "                \"model\": self.model_name,\n",
    "                \"input\": texts\n",
    "            }\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "    def get_embeddings(self, texts):\n",
    "        response = self._post_request(texts)\n",
    "        embeddings = response[\"embeddings\"]\n",
    "        # If only one embedding is returned, return it directly\n",
    "        if isinstance(texts, str) or len(embeddings) == 1:\n",
    "            return embeddings[0]\n",
    "        return embeddings\n",
    "\n",
    "    def get_full_response(self, texts):\n",
    "        return self._post_request(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_ollama = OllamaEmbedder(url=config.OAPI_EMBED_URL, model_name=config.OMODEL_NOMIC)\n",
    "\n",
    "texts = [\"Warum bin ich so fr√∂hlich?\", \"Ich liebe K√§se.\", \"Was ist k√ºnstliche Intelligenz?\"]\n",
    "\n",
    "fullresponse = embedder_ollama.get_full_response(texts)\n",
    "show_pretty_json(fullresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = embedder_ollama.get_embeddings(texts)\n",
    "\n",
    "print(len(vectors))          # Anzahl der Texte\n",
    "print(len(vectors[0]))       # Dimension des Embeddings\n",
    "print(vectors[0][:10])       # Ersten 10 Werte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAISDK-Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openai SDK Client erstellen\n",
    "client = OpenAI(\n",
    "    base_url=config.OAPI_OPENAISDK_BASE_URL,\n",
    "    api_key=config.OLLAMA_FAKE_API_KEY,  # fake key!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenaisdkEmbedder:\n",
    "    def __init__(self, client, model_name):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def _post_request(self, texts):\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # POST-Anfrage mit mehreren Texten gleichzeitig (Batch-Embedding)\n",
    "        response = self.client.embeddings.create(\n",
    "            model=self.model_name,\n",
    "            input=texts,\n",
    "            encoding_format=\"float\"\n",
    "            )\n",
    "\n",
    "        return response\n",
    "    \n",
    "    def get_embeddings(self, texts):\n",
    "        response = self._post_request(texts)\n",
    "        embeddings = [data_item.embedding for data_item in response.data]\n",
    "        # If only one embedding is returned, return it directly\n",
    "        if isinstance(texts, str) or len(embeddings) == 1:\n",
    "            return embeddings[0]\n",
    "        return embeddings\n",
    "\n",
    "    def get_full_response(self, texts):\n",
    "        return self._post_request(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder_openaisdk = OpenaisdkEmbedder(client=client, model_name=config.OMODEL_NOMIC)\n",
    "\n",
    "texts = [\"Warum bin ich so fr√∂hlich?\", \"Ich liebe K√§se.\", \"Was ist k√ºnstliche Intelligenz?\"]\n",
    "response = embedder_openaisdk.get_full_response(texts)\n",
    "\n",
    "show_pretty_json(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Warum bin ich so fr√∂hlich?\", \"Ich liebe K√§se.\", \"Was ist k√ºnstliche Intelligenz?\"]\n",
    "vectors = embedder_openaisdk.get_embeddings(texts)\n",
    "\n",
    "show_pretty_json(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèãÔ∏è **[√úBUNG_2.3.01]** Vergleich von Embedding-Werten durch Cosinus Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Diese √úbung ist Teil von √úBUNG4. Speichere deine Ergebnisse und Notizen in einem File (Word), f√ºge dann noch die restlichen √úbungen dieser Session 2.2 hinzu und lade sie auf Moodle unter \"Abgabe √úbung 4\" bis zum 20.05.25 hoch.*\n",
    "- Verwende 3 verschiedene Models\n",
    "- Datenset: Text1, Text2 (√§hnlich zu Text1), Text3 (un√§hnlich zu Text1)\n",
    "- Pro Model Cosinus Similarity der Vektoren berechnen\n",
    "- Mit weiterem 2. Datenset probieren (Texte austauschen)\n",
    "- Mit weiterem 3. Datenset probieren (Texte austauschen)\n",
    "- Erfahrungen notieren (auch die Vektor-Dimensionsanzahl eines jeden Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîì **[L√ñSUNG_2.3.01]** Vergleich von Embeddingwerten von Modellen durch Cosinus Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aiworkshop)",
   "language": "python",
   "name": "aiworkshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
